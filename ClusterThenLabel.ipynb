{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1472453,
          "sourceType": "datasetVersion",
          "datasetId": 863934
        },
        {
          "sourceId": 8117716,
          "sourceType": "datasetVersion",
          "datasetId": 4796295
        },
        {
          "sourceId": 8300652,
          "sourceType": "datasetVersion",
          "datasetId": 4931339
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification, BatchEncoding, BertConfig\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import tf_keras\n",
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "8y4JYHAZ1jTg",
        "execution": {
          "iopub.status.busy": "2024-06-30T14:17:14.731125Z",
          "iopub.execute_input": "2024-06-30T14:17:14.731417Z",
          "iopub.status.idle": "2024-06-30T14:17:46.129725Z",
          "shell.execute_reply.started": "2024-06-30T14:17:14.731391Z",
          "shell.execute_reply": "2024-06-30T14:17:46.128877Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (1)\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "train_dataset = sst2_dataset[\"train\"]\n",
        "validation_dataset = sst2_dataset[\"validation\"]\n",
        "test_dataset = sst2_dataset[\"test\"]\n",
        "train_df=pd.DataFrame(train_dataset.to_pandas())\n",
        "val_df=pd.DataFrame(validation_dataset.to_pandas())\n",
        "test_df=pd.DataFrame(test_dataset.to_pandas())\n",
        "df=pd.concat([train_df, val_df], ignore_index=True)\n",
        "df.drop(columns=['idx'], inplace=True)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "4mD-fo_ngPTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (2)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['label'], random_state=42)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.4, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=2\n",
        "print(train_df)\n",
        "print(unlab_df)\n",
        "print(val_df)\n",
        "print(test_df)\n",
        "#For the first split, put the frac value below to be equal to 0.0031 for about 100 labeled samples\n",
        "#For the second split, put the frac value below to be equal to 0.01 for about 300 labeled samples\n",
        "#For the third split, put the frac value below to be equal to 0.0153 for about 500 labeled samples\n",
        "#For the fourth split, put the frac value below to be equal to 0.031 for about 1000 labeled samples\n",
        "#For the fifth split put the frac value below to be equal to 0.0765 for about 2500 labeled samples\n",
        "#For the sixth split, put the frac value below to be equal to 0.153 for about 5000 labeled samples\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.153))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "unhH7bmcz78x",
        "execution": {
          "iopub.status.busy": "2024-06-30T14:17:56.672204Z",
          "iopub.execute_input": "2024-06-30T14:17:56.672604Z",
          "iopub.status.idle": "2024-06-30T14:18:04.925453Z",
          "shell.execute_reply.started": "2024-06-30T14:17:56.672564Z",
          "shell.execute_reply": "2024-06-30T14:18:04.924548Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (1)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "5gUmapKf0FeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (2)\n",
        "train_df=pd.read_csv('/content/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
        "test_df=pd.read_csv('/content/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
        "train_df=train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "test_df=test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "train_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "test_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "df=pd.concat([train_df, test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['Sentiment'], random_state=42)\n",
        "mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive':3, 'Extremely Positive':4}\n",
        "train_df['Sentiment']=train_df['Sentiment'].replace(mapping)\n",
        "val_df['Sentiment']=val_df['Sentiment'].replace(mapping)\n",
        "test_df['Sentiment']=test_df['Sentiment'].replace(mapping)\n",
        "train_df['Sentiment']=train_df['Sentiment'].astype('category')\n",
        "val_df['Sentiment']=val_df['Sentiment'].astype('category')\n",
        "test_df['Sentiment']=test_df['Sentiment'].astype('category')\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.42, stratify=train_df['label'], random_state=42)\n",
        "unlab_df['Sentiment']=unlab_df['Sentiment'].cat.add_categories(5)\n",
        "unlab_df['Sentiment']=5\n",
        "#For the first split, put the frac value below to be equal to 0.0051 for about 100 labeled\n",
        "#For the second split, put the frac value below to be equal to 0.0255 for about 500 labeled\n",
        "#For the third split, put the frac value below to be equal to 0.051 for about 1000 labeled\n",
        "#For the fourth split, put the frac value below to be equal to 0.2545 for about 5000 labeled\n",
        "train_df=train_df.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.0051))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "Sn8TUO_Z0IFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT SST-5 DATASET\n",
        "dataset=load_dataset(\"SetFit/sst5\")\n",
        "train_dataset=dataset['train']\n",
        "val_dataset=dataset['validation']\n",
        "test_dataset=dataset['test']\n",
        "train_df=pd.DataFrame(train_dataset)\n",
        "train_df=train_df.drop(columns=['label_text'])\n",
        "val_df=pd.DataFrame(val_dataset)\n",
        "val_df=val_df.drop(columns=['label_text'])\n",
        "test_df=pd.DataFrame(test_dataset)\n",
        "test_df=test_df.drop(columns=['label_text'])\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.53, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=5\n",
        "#For the first split, put the frac value below to be equal to 0.027 for about 100 labeled\n",
        "#For the second split, put the frac value below to be equal to 0.125 for about 500 labeled\n",
        "#For the third split, put the frac value below to be equal to 0.25 for about 1000 labeled\n",
        "#For the fourth split, put the frac value below to be equal to 0.5 for about 5000 labeled\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.125))\n",
        "train_df.drop_duplicates(subset='text',inplace=True)\n",
        "unlab_df.drop_duplicates(subset='text', inplace=True)\n",
        "val_df.drop_duplicates(subset='text',inplace=True)\n",
        "test_df.drop_duplicates(subset='text',inplace=True)\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "Yfe2NuBz0TsQ",
        "execution": {
          "iopub.status.busy": "2024-06-29T21:57:12.508291Z",
          "iopub.execute_input": "2024-06-29T21:57:12.508889Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE IMDB Movie Review DATASET\n",
        "cwd=os.getcwd()\n",
        "data=tf.keras.utils.get_file(fname =\"aclImdb.tar.gz\", origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", cache_dir= cwd, extract = True)\n",
        "data_path=os.path.join(os.path.dirname(data), 'aclImdb')\n",
        "os.listdir(data_path)\n",
        "train_dir=os.path.join(data_path, 'train')\n",
        "for file in os.listdir(train_dir):\n",
        "    file_path=os.path.join(train_dir, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path) as f:\n",
        "            first_line=f.readline().strip()\n",
        "            print(file + \":\" + first_line)\n",
        "def load_dataset(dir, typeOfData):\n",
        "    dictnry = {\"review\": [], \"sentiment\": []}\n",
        "    if typeOfData == 'labelled':\n",
        "        pos_path=os.path.join(dir, 'pos')\n",
        "        neg_path=os.path.join(dir, 'neg')\n",
        "        for file in os.listdir(pos_path):\n",
        "            file_path=os.path.join(pos_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(1)\n",
        "        for file in os.listdir(neg_path):\n",
        "            file_path=os.path.join(neg_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(0)\n",
        "    elif typeOfData == 'unsup':\n",
        "        unsup_path=os.path.join (dir, 'unsup')\n",
        "        for file in os.listdir(unsup_path):\n",
        "            file_path=os.path.join(unsup_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(2)\n",
        "    return dictnry\n",
        "Train_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'labelled'))\n",
        "Unlab_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'unsup'))\n",
        "test_dir=os.path.join(data_path, 'test')\n",
        "Test_df=pd.DataFrame.from_dict(load_dataset(test_dir, 'labelled'))\n",
        "df=pd.concat([Train_df, Test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['sentiment'], random_state=42)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.375, stratify=train_df['sentiment'], random_state=42)\n",
        "unlab_df.loc[:, 'sentiment']=2\n",
        "#For the first split, put the frac value from the train_df below to be equal to 0.004 for about 100 labeled\n",
        "#For the second split, put the frac value from the train_df below to be equal to 0.012 for about 300 labeled\n",
        "#For the third split, put the frac value from the train_df below to be equal to 0.04 for about 1000 labeled\n",
        "#For the fourth split, put the frac value from the train_df below to be equal to 0.2 for about 5000 labeled\n",
        "train_df=train_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n",
        "val_df=val_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "test_df=test_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "te68CH-a1MQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class clusterThenLabelKMeans(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.tz=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert_model=TFBertModel.from_pretrained('bert-base-uncased', num_attention_heads=12)\n",
        "        self.bertClassification=TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def cleaning(self, text):\n",
        "        soup=BeautifulSoup(text, \"html.parser\")\n",
        "        text=re.sub(r\"\\[[^]]*\\]\" , '', soup.get_text())\n",
        "        text=re.sub(r\"[^A-Za-z0-9\\s,']\", '', text)\n",
        "        return text\n",
        "\n",
        "    def get_embeddings(self, text, label, typeOfData, batch_size, max_len=64):\n",
        "        if typeOfData=='train':\n",
        "            df=train_df\n",
        "        elif typeOfData=='unlab':\n",
        "            df=unlab_df\n",
        "        elif typeOfData=='val':\n",
        "            df=val_df\n",
        "        elif typeOfData=='test':\n",
        "            df=test_df\n",
        "        df['cleaned']=df[text].apply(self.cleaning)\n",
        "        targets=df[label]\n",
        "        tokens=self.tz.batch_encode_plus(df['cleaned'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "        dataset=tf.data.Dataset.from_tensor_slices((tokens['input_ids'], tokens['attention_mask']))\n",
        "        dataset=dataset.batch(batch_size)\n",
        "        embeddings=[]\n",
        "        for batch in dataset:\n",
        "            embeddings.append(self.bert_model(batch[0], batch[1])['last_hidden_state'])\n",
        "        concatenated_embeddings=[]\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            for i in range(0, len(embeddings), batch_size):\n",
        "                batch=embeddings[i:i+batch_size]\n",
        "                concatenated_batch=tf.concat(batch, axis=0)\n",
        "                concatenated_embeddings.append(concatenated_batch)\n",
        "            embeddings=tf.concat(concatenated_embeddings, axis=0)\n",
        "        targets=tf.convert_to_tensor(targets)\n",
        "        return tokens, embeddings[:,0], targets\n",
        "\n",
        "    def choosing_num_clusters(self, data):\n",
        "        #FOR A NUMBER OF CLUSTERS YOU WANT, YOU SUBTRACT TWO. SO IF YOU WANT 2 CLUSTERS, YOU PUT THE VALUE 0 IN num_clusters\n",
        "        num_clusters=1\n",
        "        return num_clusters\n",
        "\n",
        "    def compile(self, optimizer, loss, metrics):\n",
        "        super().compile()\n",
        "        self.loss=loss\n",
        "        self.opt=optimizer\n",
        "        self.mtrc=metrics\n",
        "        return self.loss, self.opt, self.mtrc\n",
        "\n",
        "    def clusteringKMeans(self, labeled, unlabeled, batch_size):\n",
        "        self.numLabeled=labeled[1].shape[0]\n",
        "        self.numUnlabeled=unlabeled[1].shape[0]\n",
        "        with tf.device(\"/CPU:0\"):\n",
        "            labeled_input_ids_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(labeled[0]['input_ids']).batch(batch_size)), axis=0)\n",
        "            labeled_attention_mask_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(labeled[0]['attention_mask']).batch(batch_size)), axis=0)\n",
        "            labeled_fullEmbeddedData_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(labeled[1]).batch(batch_size)), axis=0)\n",
        "            labeled_fullTargets_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(labeled[2]).batch(batch_size)), axis=0)\n",
        "            unlabeled_input_ids_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(unlabeled[0]['input_ids']).batch(batch_size)), axis=0)\n",
        "            unlabeled_attention_mask_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(unlabeled[0]['attention_mask']).batch(batch_size)), axis=0)\n",
        "            unlabeled_fullEmbeddedData_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(unlabeled[1]).batch(batch_size)), axis=0)\n",
        "            unlabeled_fullTargets_batched = tf.concat(list(tf.data.Dataset.from_tensor_slices(unlabeled[2]).batch(batch_size)), axis=0)\n",
        "            inputIds=tf.concat([labeled_input_ids_batched,  unlabeled_input_ids_batched], axis=0)\n",
        "            attentionMask=tf.concat([labeled_attention_mask_batched, unlabeled_attention_mask_batched], axis=0)\n",
        "            fullEmbeddedData=tf.concat([labeled_fullEmbeddedData_batched, unlabeled_fullEmbeddedData_batched], axis=0)\n",
        "            fullTargets=tf.concat([labeled_fullTargets_batched, unlabeled_fullTargets_batched], axis=0)\n",
        "        k=self.choosing_num_clusters(fullEmbeddedData)\n",
        "        kmeans=KMeans(n_clusters=k+2, random_state=42, n_init='auto').fit(fullEmbeddedData)\n",
        "        num_clusters=k+2\n",
        "        return kmeans.labels_, num_clusters, inputIds, attentionMask, fullTargets\n",
        "\n",
        "    def organizingClusters(self, cluster_labels, num_clusters, input_ids, attention_mask, allTargets):\n",
        "        clusterIndices=[]\n",
        "        clustersOGLabels=[]\n",
        "        clusterInputIds=[]\n",
        "        clusterAttentionMask=[]\n",
        "        labeledClusterIndices=[]\n",
        "        unlabeledClusterIndices=[]\n",
        "        labeledClusterInputIds=[]\n",
        "        labeledClusterAttentionMask=[]\n",
        "        labeledClusterLabels=[]\n",
        "        unlabeledClusterInputIds=[]\n",
        "        unlabeledClusterAttentionMask=[]\n",
        "        unlabeledClusterLabels=[]\n",
        "        num_classes=train_df[train_df.columns[1]].nunique()\n",
        "        possible_cluster_labels=[i for i in range(num_clusters)]\n",
        "        for i in possible_cluster_labels:\n",
        "            clusterIndices.append(np.where(cluster_labels==i)[0].tolist())\n",
        "            clustersOGLabels.append(tf.gather(allTargets, clusterIndices[i]))\n",
        "            labeledClusterIndices.append(np.where(clustersOGLabels[i]!=num_classes)[0].tolist())\n",
        "            unlabeledClusterIndices.append(np.where(clustersOGLabels[i]==num_classes)[0].tolist())\n",
        "            clusterInputIds.append(tf.gather(input_ids, clusterIndices[i]))\n",
        "            clusterAttentionMask.append(tf.gather(attention_mask, clusterIndices[i]))\n",
        "            labeledClusterInputIds.append(tf.gather(clusterInputIds[i], labeledClusterIndices[i]))\n",
        "            labeledClusterAttentionMask.append(tf.gather(clusterAttentionMask[i], labeledClusterIndices[i]))\n",
        "            labeledClusterLabels.append(tf.gather(clustersOGLabels[i], labeledClusterIndices[i]))\n",
        "            unlabeledClusterInputIds.append(tf.gather(clusterInputIds[i], unlabeledClusterIndices[i]))\n",
        "            unlabeledClusterAttentionMask.append(tf.gather(clusterAttentionMask[i], unlabeledClusterIndices[i]))\n",
        "            unlabeledClusterLabels.append(tf.gather(clustersOGLabels[i], unlabeledClusterIndices[i]))\n",
        "        return clusterIndices, clusterInputIds, clusterAttentionMask, clustersOGLabels, labeledClusterInputIds, labeledClusterAttentionMask, labeledClusterLabels, unlabeledClusterInputIds, unlabeledClusterAttentionMask, unlabeledClusterLabels"
      ],
      "metadata": {
        "id": "3OIyf47r1jT4",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:21:52.141858Z",
          "iopub.execute_input": "2024-06-30T15:21:52.142188Z",
          "iopub.status.idle": "2024-06-30T15:21:52.171592Z",
          "shell.execute_reply.started": "2024-06-30T15:21:52.142162Z",
          "shell.execute_reply": "2024-06-30T15:21:52.170598Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ex=clusterThenLabelKMeans()"
      ],
      "metadata": {
        "id": "6tXtg4ezMIRD",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:21:53.088912Z",
          "iopub.execute_input": "2024-06-30T15:21:53.089564Z",
          "iopub.status.idle": "2024-06-30T15:21:55.477111Z",
          "shell.execute_reply.started": "2024-06-30T15:21:53.089535Z",
          "shell.execute_reply": "2024-06-30T15:21:55.476193Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train=ex.get_embeddings(train_df.columns[0], train_df.columns[1], 'train', 32)\n",
        "print(\"DONE\")\n",
        "unlabeled=ex.get_embeddings(unlab_df.columns[0], unlab_df.columns[1], 'unlab', 32)\n",
        "print(\"DONE\")\n",
        "val=ex.get_embeddings(val_df.columns[0], val_df.columns[1], 'val', 32)\n",
        "print(\"DONE\")\n",
        "test=ex.get_embeddings(test_df.columns[0], test_df.columns[1], 'test', 32)\n",
        "unique, _=tf.unique(train[2])\n",
        "num_classes=unique.shape[0]"
      ],
      "metadata": {
        "id": "-gcNa37Hlnwa",
        "execution": {
          "iopub.status.busy": "2024-06-30T14:18:24.185695Z",
          "iopub.execute_input": "2024-06-30T14:18:24.186050Z",
          "iopub.status.idle": "2024-06-30T14:22:28.265078Z",
          "shell.execute_reply.started": "2024-06-30T14:18:24.186023Z",
          "shell.execute_reply": "2024-06-30T14:22:28.264068Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A=ex.clusteringKMeans(train, unlabeled, 4)"
      ],
      "metadata": {
        "id": "E_cOj3Jz1jT7",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:21:57.848532Z",
          "iopub.execute_input": "2024-06-30T15:21:57.848918Z",
          "iopub.status.idle": "2024-06-30T15:22:01.536616Z",
          "shell.execute_reply.started": "2024-06-30T15:21:57.848885Z",
          "shell.execute_reply": "2024-06-30T15:22:01.535586Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B=ex.organizingClusters(A[0], A[1], A[2], A[3], A[4])"
      ],
      "metadata": {
        "id": "X2dU1Pf41jT7",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:22:02.933702Z",
          "iopub.execute_input": "2024-06-30T15:22:02.934568Z",
          "iopub.status.idle": "2024-06-30T15:22:02.999194Z",
          "shell.execute_reply.started": "2024-06-30T15:22:02.934536Z",
          "shell.execute_reply": "2024-06-30T15:22:02.998456Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if num_classes == 2:\n",
        "    loss=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif num_classes == 5:\n",
        "    loss=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "#YOU SET THE PATIENCE VALUE HERE\n",
        "callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "W0UDmimLPS9X",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:22:04.137871Z",
          "iopub.execute_input": "2024-06-30T15:22:04.138409Z",
          "iopub.status.idle": "2024-06-30T15:22:04.147677Z",
          "shell.execute_reply.started": "2024-06-30T15:22:04.138380Z",
          "shell.execute_reply": "2024-06-30T15:22:04.146590Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valY=tf.one_hot(val[2], depth=num_classes)\n",
        "testY=tf.one_hot(test[2], depth=num_classes)\n",
        "history=[]\n",
        "trainY=[]\n",
        "unlabPseudoLabels=[]\n",
        "initial_lr=1e-5\n",
        "for i in range(A[1]):\n",
        "    if i == 0:\n",
        "        opt=tf_keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
        "    elif i == 1:\n",
        "        opt=tf_keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
        "    print(i)\n",
        "    trainY.append(tf.one_hot(B[6][i], depth=num_classes))\n",
        "    num_epochs=100\n",
        "    config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "    config.num_labels=num_classes\n",
        "    config.hidden_dropout_prob=0.3\n",
        "    model=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "    model.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
        "    history.append(model.fit([B[4][i], B[5][i]], trainY[i], batch_size=32, epochs=num_epochs, verbose=1, shuffle=True, callbacks=[callback], validation_data=([val[0]['input_ids'], val[0]['attention_mask']], valY)))\n",
        "    predY=tf.convert_to_tensor(model.predict([B[7][i], B[8][i]])[0])\n",
        "    if num_classes == 2:\n",
        "        unlabPseudoLabels.append(tf.argmax(tf.nn.sigmoid(predY), axis=1))\n",
        "    elif num_classes == 5:\n",
        "        unlabPseudoLabels.append(tf.argmax(tf.nn.softmax(predY, axis=1), axis=1))\n",
        "    print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")"
      ],
      "metadata": {
        "id": "NV2CkipAPS9Y",
        "execution": {
          "iopub.status.busy": "2024-06-30T15:22:10.319469Z",
          "iopub.execute_input": "2024-06-30T15:22:10.320060Z",
          "iopub.status.idle": "2024-06-30T16:03:35.432222Z",
          "shell.execute_reply.started": "2024-06-30T15:22:10.320027Z",
          "shell.execute_reply": "2024-06-30T16:03:35.431277Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataInputIds=[]\n",
        "dataAttentionMask=[]\n",
        "dataY=[]\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    for i in range(A[1]):\n",
        "        dataInputIds.append(tf.concat([B[4][i], B[7][i]], axis=0))\n",
        "        dataAttentionMask.append(tf.concat([B[5][i], B[8][i]], axis=0))\n",
        "        dataY.append(tf.concat([B[6][i], unlabPseudoLabels[i]], axis=0))\n",
        "    dataInputIds=tf.concat(dataInputIds, axis=0)\n",
        "    dataAttentionMask=tf.concat(dataAttentionMask, axis=0)\n",
        "    dataY=tf.one_hot(tf.concat(dataY, axis=0), depth=num_classes)"
      ],
      "metadata": {
        "id": "_eMGOlr61jT8",
        "execution": {
          "iopub.status.busy": "2024-06-30T16:50:35.437997Z",
          "iopub.execute_input": "2024-06-30T16:50:35.438837Z",
          "iopub.status.idle": "2024-06-30T16:50:35.450740Z",
          "shell.execute_reply.started": "2024-06-30T16:50:35.438780Z",
          "shell.execute_reply": "2024-06-30T16:50:35.449770Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_lr=8e-6\n",
        "opt=tf_keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
        "if num_classes == 2:\n",
        "    loss=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif num_classes == 5:\n",
        "    loss=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "30MjZ7PY1jT9",
        "execution": {
          "iopub.status.busy": "2024-06-30T17:26:57.787330Z",
          "iopub.execute_input": "2024-06-30T17:26:57.788291Z",
          "iopub.status.idle": "2024-06-30T17:26:57.812023Z",
          "shell.execute_reply.started": "2024-06-30T17:26:57.788258Z",
          "shell.execute_reply": "2024-06-30T17:26:57.811264Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.num_labels=num_classes\n",
        "config.hidden_dropout_prob=0.2\n",
        "modelFull=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "modelFull.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
        "historyFull=modelFull.fit([dataInputIds, dataAttentionMask], dataY, batch_size=32, epochs=num_epochs, verbose=1, shuffle=True, callbacks=[callback], validation_data=([val[0]['input_ids'], val[0]['attention_mask']], valY))"
      ],
      "metadata": {
        "id": "5f8bL8ki1jT9",
        "execution": {
          "iopub.status.busy": "2024-06-30T17:26:58.746672Z",
          "iopub.execute_input": "2024-06-30T17:26:58.747385Z",
          "iopub.status.idle": "2024-06-30T17:49:08.527881Z",
          "shell.execute_reply.started": "2024-06-30T17:26:58.747356Z",
          "shell.execute_reply": "2024-06-30T17:49:08.527059Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = modelFull.evaluate([test[0]['input_ids'], test[0]['attention_mask']], testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "y43gt6J-1jT-",
        "execution": {
          "iopub.status.busy": "2024-06-30T17:49:59.127537Z",
          "iopub.execute_input": "2024-06-30T17:49:59.127939Z",
          "iopub.status.idle": "2024-06-30T17:50:40.150433Z",
          "shell.execute_reply.started": "2024-06-30T17:49:59.127907Z",
          "shell.execute_reply": "2024-06-30T17:50:40.149477Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sup_trainY=tf.one_hot(train[2], depth=num_classes)\n",
        "sup_valY=tf.one_hot(val[2], depth=num_classes)\n",
        "sup_testY=tf.one_hot(test[2], depth=num_classes)"
      ],
      "metadata": {
        "id": "TkdYXRoG1jT-",
        "execution": {
          "iopub.status.busy": "2024-06-10T16:17:56.744152Z",
          "iopub.execute_input": "2024-06-10T16:17:56.744539Z",
          "iopub.status.idle": "2024-06-10T16:17:56.751159Z",
          "shell.execute_reply.started": "2024-06-10T16:17:56.744508Z",
          "shell.execute_reply": "2024-06-10T16:17:56.750335Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HERE YOU RUN THE SUPERVISED MODEL\n",
        "num_epochs=100\n",
        "config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.hidden_dropout_prob=0.3\n",
        "config.num_labels=num_classes\n",
        "model=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "initial_lr=3e-5\n",
        "#lr_schedule = tf_keras.optimizers.schedules.ExponentialDecay(initial_lr, decay_steps=500, decay_rate=0.9)\n",
        "opt=tf_keras.optimizers.legacy.Adam(learning_rate=initial_lr)\n",
        "if num_classes == 2:\n",
        "    loss=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif num_classes == 5:\n",
        "    loss=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=7, restore_best_weights=True)\n",
        "#reduce_lr=tf_keras.callbacks.ReduceLROnPlateau(monitor='val_loss', mode=\"min\", factor=0.7, patience=1, min_lr=2e-6)\n",
        "model.compile(optimizer=opt, loss=loss, metrics=[metric])\n",
        "history=model.fit([train[0]['input_ids'], train[0]['attention_mask']], sup_trainY, batch_size=32, epochs=num_epochs, shuffle=True, callbacks=[callback], verbose=1, validation_data=([val[0]['input_ids'], val[0]['attention_mask']], sup_valY))"
      ],
      "metadata": {
        "id": "eKn5niPE1jT-",
        "execution": {
          "iopub.status.busy": "2024-06-10T16:18:20.892637Z",
          "iopub.execute_input": "2024-06-10T16:18:20.893399Z",
          "iopub.status.idle": "2024-06-10T16:24:59.608652Z",
          "shell.execute_reply.started": "2024-06-10T16:18:20.893356Z",
          "shell.execute_reply": "2024-06-10T16:24:59.607797Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate([test[0]['input_ids'], test[0]['attention_mask']], sup_testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "FTTlLsOuPS9Z",
        "execution": {
          "iopub.status.busy": "2024-06-10T16:28:33.340025Z",
          "iopub.execute_input": "2024-06-10T16:28:33.340406Z",
          "iopub.status.idle": "2024-06-10T16:28:55.780433Z",
          "shell.execute_reply.started": "2024-06-10T16:28:33.340373Z",
          "shell.execute_reply": "2024-06-10T16:28:55.779518Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}