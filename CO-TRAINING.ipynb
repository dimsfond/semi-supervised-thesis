{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 8001943,
          "sourceType": "datasetVersion",
          "datasetId": 4712261
        },
        {
          "sourceId": 8117716,
          "sourceType": "datasetVersion",
          "datasetId": 4796295
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification, BatchEncoding, BertConfig\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.decomposition import PCA\n",
        "import tf_keras\n",
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import BatchEncoding\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "nXMUWtXVKGcQ",
        "execution": {
          "iopub.status.busy": "2024-07-07T23:39:04.548204Z",
          "iopub.execute_input": "2024-07-07T23:39:04.548518Z",
          "iopub.status.idle": "2024-07-07T23:39:36.078439Z",
          "shell.execute_reply.started": "2024-07-07T23:39:04.548490Z",
          "shell.execute_reply": "2024-07-07T23:39:36.077398Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (1)\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "train_dataset = sst2_dataset[\"train\"]\n",
        "validation_dataset = sst2_dataset[\"validation\"]\n",
        "test_dataset = sst2_dataset[\"test\"]\n",
        "train_df=pd.DataFrame(train_dataset.to_pandas())\n",
        "val_df=pd.DataFrame(validation_dataset.to_pandas())\n",
        "test_df=pd.DataFrame(test_dataset.to_pandas())\n",
        "df=pd.concat([train_df, val_df], ignore_index=True)\n",
        "df.drop(columns=['idx'], inplace=True)\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-07T23:39:42.085992Z",
          "iopub.execute_input": "2024-07-07T23:39:42.086793Z",
          "iopub.status.idle": "2024-07-07T23:39:46.022511Z",
          "shell.execute_reply.started": "2024-07-07T23:39:42.086743Z",
          "shell.execute_reply": "2024-07-07T23:39:46.021193Z"
        },
        "trusted": true,
        "id": "oA8gYrYXLtAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (2)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['label'], random_state=5)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['label'], random_state=5)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.4, stratify=train_df['label'], random_state=5)\n",
        "unlab_df.loc[:, 'label']=2\n",
        "print(train_df)\n",
        "print(unlab_df)\n",
        "print(val_df)\n",
        "print(test_df)\n",
        "#For the first split, put the frac value below to be equal to 0.0031 for about 100 labeled samples\n",
        "#For the second split, put the frac value below to be equal to 0.01 for about 300 labeled samples\n",
        "#For the third split, put the frac value below to be equal to 0.0153 for about 500 labeled samples\n",
        "#For the fourth split, put the frac value below to be equal to 0.031 for about 1000 labeled samples\n",
        "#For the fifth split put the frac value below to be equal to 0.0765 for about 2500 labeled samples\n",
        "#For the sixth split, put the frac value below to be equal to 0.153 for about 5000 labeled samples\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.0153))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-07T23:39:50.699459Z",
          "iopub.execute_input": "2024-07-07T23:39:50.699926Z",
          "iopub.status.idle": "2024-07-07T23:39:50.788768Z",
          "shell.execute_reply.started": "2024-07-07T23:39:50.699889Z",
          "shell.execute_reply": "2024-07-07T23:39:50.787860Z"
        },
        "trusted": true,
        "id": "_uNMJkNFLtAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE IMDB Movie Review DATASET\n",
        "cwd=os.getcwd()\n",
        "data=tf.keras.utils.get_file(fname =\"aclImdb.tar.gz\", origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", cache_dir= cwd, extract = True)\n",
        "data_path=os.path.join(os.path.dirname(data), 'aclImdb')\n",
        "os.listdir(data_path)\n",
        "train_dir=os.path.join(data_path, 'train')\n",
        "for file in os.listdir(train_dir):\n",
        "    file_path=os.path.join(train_dir, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path) as f:\n",
        "            first_line=f.readline().strip()\n",
        "            print(file + \":\" + first_line)\n",
        "def load_dataset(dir, typeOfData):\n",
        "    dictnry = {\"review\": [], \"sentiment\": []}\n",
        "    if typeOfData == 'labelled':\n",
        "        pos_path=os.path.join(dir, 'pos')\n",
        "        neg_path=os.path.join(dir, 'neg')\n",
        "        for file in os.listdir(pos_path):\n",
        "            file_path=os.path.join(pos_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(1)\n",
        "        for file in os.listdir(neg_path):\n",
        "            file_path=os.path.join(neg_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(0)\n",
        "    elif typeOfData == 'unsup':\n",
        "        unsup_path=os.path.join (dir, 'unsup')\n",
        "        for file in os.listdir(unsup_path):\n",
        "            file_path=os.path.join(unsup_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(2)\n",
        "    return dictnry\n",
        "Train_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'labelled'))\n",
        "Unlab_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'unsup'))\n",
        "test_dir=os.path.join(data_path, 'test')\n",
        "Test_df=pd.DataFrame.from_dict(load_dataset(test_dir, 'labelled'))\n",
        "df=pd.concat([Train_df, Test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['sentiment'], random_state=42)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.375, stratify=train_df['sentiment'], random_state=42)\n",
        "unlab_df.loc[:, 'sentiment']=2\n",
        "#For the first split, put the frac value from the train_df below to be equal to 0.004 for about 100 labeled\n",
        "#For the second split, put the frac value from the train_df below to be equal to 0.012 for about 300 labeled\n",
        "#For the third split, put the frac value from the train_df below to be equal to 0.04 for about 1000 labeled\n",
        "#For the fourth split, put the frac value from the train_df below to be equal to 0.2 for about 5000 labeled\n",
        "train_df=train_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n",
        "val_df=val_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "test_df=test_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "_rD80hnh1szd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (1)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "rSmIETjh-_sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (2)\n",
        "train_df=pd.read_csv('/content/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
        "test_df=pd.read_csv('/content/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
        "train_df=train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "test_df=test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "train_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "test_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "df=pd.concat([train_df, test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['Sentiment'], random_state=42)\n",
        "mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive':3, 'Extremely Positive':4}\n",
        "train_df['Sentiment']=train_df['Sentiment'].replace(mapping)\n",
        "val_df['Sentiment']=val_df['Sentiment'].replace(mapping)\n",
        "test_df['Sentiment']=test_df['Sentiment'].replace(mapping)\n",
        "train_df['Sentiment']=train_df['Sentiment'].astype('category')\n",
        "val_df['Sentiment']=val_df['Sentiment'].astype('category')\n",
        "test_df['Sentiment']=test_df['Sentiment'].astype('category')\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.42, stratify=train_df['label'], random_state=42)\n",
        "unlab_df['Sentiment']=unlab_df['Sentiment'].cat.add_categories(5)\n",
        "unlab_df['Sentiment']=5\n",
        "#For the first split, put the frac value below to be equal to 0.0051\n",
        "#For the second split, put the frac value below to be equal to 0.0255\n",
        "#For the third split, put the frac value below to be equal to 0.051\n",
        "#For the fourth split, put the frac value below to be equal to 0.2545\n",
        "train_df=train_df.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.0051))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "9aqh2Kzyuzh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT SST-5 DATASET\n",
        "dataset=load_dataset(\"SetFit/sst5\")\n",
        "train_dataset=dataset['train']\n",
        "val_dataset=dataset['validation']\n",
        "test_dataset=dataset['test']\n",
        "train_df=pd.DataFrame(train_dataset)\n",
        "train_df=train_df.drop(columns=['label_text'])\n",
        "val_df=pd.DataFrame(val_dataset)\n",
        "val_df=val_df.drop(columns=['label_text'])\n",
        "test_df=pd.DataFrame(test_dataset)\n",
        "test_df=test_df.drop(columns=['label_text'])\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.53, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=5\n",
        "#For the first split, put the frac value below to be equal to 0.027\n",
        "#For the second split, put the frac value below to be equal to 0.125\n",
        "#For the third split, put the frac value below to be equal to 0.25\n",
        "#For the fourth split, put the frac value below to be equal to 0.5\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.125))\n",
        "train_df.drop_duplicates(subset='text',inplace=True)\n",
        "unlab_df.drop_duplicates(subset='text', inplace=True)\n",
        "val_df.drop_duplicates(subset='text',inplace=True)\n",
        "test_df.drop_duplicates(subset='text',inplace=True)\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "P6K5RGBpvM2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanText(text):\n",
        "    soup=BeautifulSoup(text, \"html.parser\")\n",
        "    text=re.sub(r\"\\[[^]]*\\]\" , '', soup.get_text())\n",
        "    text=re.sub(r\"[^A-Za-z0-9\\s,']\", '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "nLDmBtD3KGcZ",
        "execution": {
          "iopub.status.busy": "2024-07-08T01:07:15.629482Z",
          "iopub.execute_input": "2024-07-08T01:07:15.630204Z",
          "iopub.status.idle": "2024-07-08T01:07:15.635066Z",
          "shell.execute_reply.started": "2024-07-08T01:07:15.630171Z",
          "shell.execute_reply": "2024-07-08T01:07:15.633976Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['cleaned']=train_df[train_df.columns[0]].apply(cleanText)\n",
        "unlab_df['cleaned']=unlab_df[train_df.columns[0]].apply(cleanText)\n",
        "val_df['cleaned']=val_df[train_df.columns[0]].apply(cleanText)\n",
        "test_df['cleaned']=test_df[train_df.columns[0]].apply(cleanText)"
      ],
      "metadata": {
        "id": "euhNmR9zvwCa",
        "execution": {
          "iopub.status.busy": "2024-07-08T01:07:16.602440Z",
          "iopub.execute_input": "2024-07-08T01:07:16.603137Z",
          "iopub.status.idle": "2024-07-08T01:07:18.501494Z",
          "shell.execute_reply.started": "2024-07-08T01:07:16.603097Z",
          "shell.execute_reply": "2024-07-08T01:07:18.500689Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=64\n",
        "tz=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "train_encoded=tz.batch_encode_plus(train_df['cleaned'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "unlab_encoded=tz.batch_encode_plus(unlab_df['cleaned'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "val_encoded=tz.batch_encode_plus(val_df['cleaned'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "test_encoded=tz.batch_encode_plus(test_df['cleaned'].tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')"
      ],
      "metadata": {
        "id": "ijsBUepKKGcb",
        "execution": {
          "iopub.status.busy": "2024-07-08T01:07:19.393383Z",
          "iopub.execute_input": "2024-07-08T01:07:19.394086Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-07T23:40:17.636700Z",
          "iopub.execute_input": "2024-07-07T23:40:17.637603Z",
          "iopub.status.idle": "2024-07-07T23:40:17.642690Z",
          "shell.execute_reply.started": "2024-07-07T23:40:17.637571Z",
          "shell.execute_reply": "2024-07-07T23:40:17.641734Z"
        },
        "trusted": true,
        "id": "uOqWK7afLtA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TR1=train_encoded['input_ids']\n",
        "TR2=train_encoded['attention_mask']\n",
        "U1=unlab_encoded['input_ids']\n",
        "U2=unlab_encoded['attention_mask']\n",
        "V1=val_encoded['input_ids']\n",
        "V2=val_encoded['attention_mask']\n",
        "TS1=test_encoded['input_ids']\n",
        "TS2=test_encoded['attention_mask']"
      ],
      "metadata": {
        "id": "uNf48EqMhWlj",
        "execution": {
          "iopub.status.busy": "2024-07-07T23:40:18.579292Z",
          "iopub.execute_input": "2024-07-07T23:40:18.579912Z",
          "iopub.status.idle": "2024-07-07T23:40:18.586946Z",
          "shell.execute_reply.started": "2024-07-07T23:40:18.579881Z",
          "shell.execute_reply": "2024-07-07T23:40:18.584428Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY=tf.convert_to_tensor(train_df[train_df.columns[1]])\n",
        "unique, _=tf.unique(trainY)\n",
        "trainY=tf.one_hot(trainY, depth=unique.shape[0])\n",
        "unlabY=tf.convert_to_tensor(unlab_df[unlab_df.columns[1]])\n",
        "valY=tf.convert_to_tensor(val_df[val_df.columns[1]])\n",
        "valY=tf.one_hot(valY, depth=unique.shape[0])\n",
        "testY=tf.convert_to_tensor(test_df[test_df.columns[1]])\n",
        "testY=tf.one_hot(testY, depth=unique.shape[0])"
      ],
      "metadata": {
        "id": "baI5zPwFv8rX",
        "execution": {
          "iopub.status.busy": "2024-07-07T23:40:19.539777Z",
          "iopub.execute_input": "2024-07-07T23:40:19.540362Z",
          "iopub.status.idle": "2024-07-07T23:40:19.614808Z",
          "shell.execute_reply.started": "2024-07-07T23:40:19.540331Z",
          "shell.execute_reply": "2024-07-07T23:40:19.614047Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(tf_keras.Model):\n",
        "    def __init__(self, num_classes=trainY.shape[1]):\n",
        "        super().__init__()\n",
        "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased', num_attention_heads=12)\n",
        "        #j, the current number of iteration\n",
        "        iter=1 #the number of iterations for which the first dropout value will be applied. You change it according to the experiments in the pdf\n",
        "        if j < iter:\n",
        "            self.dropout=tf_keras.layers.Dropout(0.2)\n",
        "        elif j >= iter:\n",
        "            self.dropout=tf_keras.layers.Dropout(0.2)\n",
        "        self.layer1=tf_keras.layers.Dense(units=num_classes)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        inputIds, attentionMask=inputs\n",
        "        embeddings=self.bert_model(inputIds, attentionMask)['last_hidden_state']\n",
        "        x=embeddings[:, 0]\n",
        "        if clsCount == 1:\n",
        "            x=tf.gather(x, random_indexes[0:random_indexes.shape[0]//2], axis=1)\n",
        "            print(x)\n",
        "        elif clsCount == 2:\n",
        "            x=tf.gather(x, random_indexes[random_indexes.shape[0]//2:random_indexes.shape[0]], axis=1)\n",
        "            print(x)\n",
        "        pooled_output=x\n",
        "        if training:\n",
        "            pooled_output = self.dropout(x, training=training)\n",
        "        logits = self.layer1(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "b9ddpuILFUTD",
        "execution": {
          "iopub.status.busy": "2024-07-07T23:40:21.994390Z",
          "iopub.execute_input": "2024-07-07T23:40:21.994757Z",
          "iopub.status.idle": "2024-07-07T23:40:22.004801Z",
          "shell.execute_reply.started": "2024-07-07T23:40:21.994728Z",
          "shell.execute_reply": "2024-07-07T23:40:22.003804Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global random_indexes\n",
        "global clsCount #the variable which controls which view the current model, we have defined, takes\n",
        "random_indexes=tf.random.shuffle(tf.range(1, 769))\n",
        "global j\n",
        "j=0 # the current iteration\n",
        "ogSize=U1.shape[0]\n",
        "print(ogSize)\n",
        "while U1.shape[0] != 0:\n",
        "    model1=CustomModel()\n",
        "    model2=CustomModel()\n",
        "    num_epochs=100\n",
        "    if trainY.shape[1] == 2:\n",
        "        loss1=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        loss2=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        metric1=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "        metric2=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "    elif trainY.shape[1] == 5:\n",
        "        loss1=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss2=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        metric1=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "        metric2=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "    if j == 0:\n",
        "        initial_lr=1e-5\n",
        "        opt1=tf_keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "        opt2=tf_keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "        callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "    if j > 0:\n",
        "        lr=9e-6\n",
        "        opt1=tf_keras.optimizers.Adam(learning_rate=lr)\n",
        "        opt2=tf_keras.optimizers.Adam(learning_rate=lr)\n",
        "        callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    model1.compile(optimizer=opt1, loss=loss1, metrics=[metric1])\n",
        "    model2.compile(optimizer=opt2, loss=loss2, metrics=[metric2])\n",
        "    clsCount=1\n",
        "    model1.fit([TR1, TR2], trainY, batch_size=32, epochs=50, shuffle=True, callbacks=[callback], verbose=1, validation_data=([V1, V2], valY))\n",
        "    logits1=model1.predict([U1, U2])\n",
        "    logits1=tf.convert_to_tensor(logits1)\n",
        "    if trainY.shape[1] == 2:\n",
        "        prob1=tf.reduce_max(tf.nn.sigmoid(logits1), axis=1)\n",
        "        pred1=tf.one_hot(tf.argmax(tf.nn.sigmoid(logits1), axis=1), depth=trainY.shape[1])\n",
        "    elif trainY.shape[1] == 5:\n",
        "        prob1=tf.reduce_max(tf.nn.softmax(logits1, axis=1), axis=1)\n",
        "        pred1=tf.one_hot(tf.argmax(tf.nn.softmax(logits1, axis=1), axis=1), depth=trainY.shape[1])\n",
        "    clsCount=2\n",
        "    model2.fit([TR1, TR2], trainY, batch_size=32, epochs=50, shuffle=True, callbacks=[callback], verbose=1, validation_data=([V1, V2], valY))\n",
        "    logits2=model2.predict([U1, U2])\n",
        "    logits2=tf.convert_to_tensor(logits2)\n",
        "    if trainY.shape[1] == 2:\n",
        "        prob2=tf.reduce_max(tf.nn.sigmoid(logits2), axis=1)\n",
        "        pred2=tf.one_hot(tf.argmax(tf.nn.sigmoid(logits2), axis=1), depth=trainY.shape[1])\n",
        "    elif trainY.shape[1] == 5:\n",
        "        prob2=tf.reduce_max(tf.nn.softmax(logits2, axis=1), axis=1)\n",
        "        pred2=tf.one_hot(tf.argmax(tf.nn.softmax(logits2, axis=1), axis=1), depth=trainY.shape[1])\n",
        "    indexes=[]\n",
        "    count=0\n",
        "    p=0.85 #the threshold which the max predicted probability of a sample must overcome so that it be added to the training data\n",
        "    for i in range(prob1.shape[0]):\n",
        "        if trainY.shape[1] == 2:\n",
        "            logits1_after_activ=tf.nn.sigmoid(logits1[i])\n",
        "            logits2_after_activ=tf.nn.sigmoid(logits2[i])\n",
        "        elif trainY.shape[1] == 5:\n",
        "            logits1_after_activ=tf.nn.softmax(logits1[i])\n",
        "            logits2_after_activ=tf.nn.softmax(logits2[i])\n",
        "        if (prob1[i] >= p or prob2[i] >= p):\n",
        "            if (prob1[i] >= p and prob2[i] >= p):\n",
        "                if tf.argmax(logits1_after_activ).numpy() == tf.argmax(logits2_after_activ).numpy():\n",
        "                    count+=1\n",
        "                    indexes.append(i)\n",
        "                    TR1=tf.concat([TR1, tf.reshape(U1[i], (1, U1[i].shape[0]))], axis=0)\n",
        "                    TR2=tf.concat([TR2, tf.reshape(U2[i], (1, U2[i].shape[0]))], axis=0)\n",
        "                    trainY=tf.concat([trainY, tf.reshape(pred1[i], (1, trainY.shape[1]))], axis=0)\n",
        "            else:\n",
        "                count+=1\n",
        "                indexes.append(i)\n",
        "                TR1=tf.concat([TR1, tf.reshape(U1[i], (1, U1[i].shape[0]))], axis=0)\n",
        "                TR2=tf.concat([TR2, tf.reshape(U2[i], (1, U2[i].shape[0]))], axis=0)\n",
        "                if prob1[i] > prob2[i]:\n",
        "                    trainY=tf.concat([trainY, tf.reshape(pred1[i], (1, trainY.shape[1]))], axis=0)\n",
        "                elif prob2[i] >= prob1[i]:\n",
        "                    trainY=tf.concat([trainY, tf.reshape(pred2[i], (1, trainY.shape[1]))], axis=0)\n",
        "    print(TR1.shape)\n",
        "    print(trainY.shape)\n",
        "    if len(indexes) != 0:\n",
        "        maskX=tf.ones(U1.shape[0], tf.bool)\n",
        "        maskY=tf.ones(unlabY.shape[0], tf.bool)\n",
        "        indices_to_update=tf.expand_dims(indexes, axis=1)\n",
        "        updatesX=tf.zeros_like(indexes, dtype=tf.bool)\n",
        "        updatesY=tf.zeros_like(indexes, dtype=tf.bool)\n",
        "        maskX=tf.tensor_scatter_nd_update(maskX, indices_to_update, updatesX)\n",
        "        maskY=tf.tensor_scatter_nd_update(maskY, indices_to_update, updatesY)\n",
        "        U1=tf.boolean_mask(U1, maskX)\n",
        "        U2=tf.boolean_mask(U2, maskX)\n",
        "        unlabY=tf.boolean_mask(unlabY, maskY)\n",
        "        print(U1.shape)\n",
        "    print(count, \"samples were added\")\n",
        "    j+=1\n",
        "    if j == 3:\n",
        "        break"
      ],
      "metadata": {
        "id": "e_gaNRqoKGcn",
        "execution": {
          "iopub.status.busy": "2024-07-07T23:40:25.252916Z",
          "iopub.execute_input": "2024-07-07T23:40:25.253286Z",
          "iopub.status.idle": "2024-07-08T01:04:58.296876Z",
          "shell.execute_reply.started": "2024-07-07T23:40:25.253256Z",
          "shell.execute_reply": "2024-07-08T01:04:58.295872Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW\n",
        "clsCount=1\n",
        "if trainY.shape[1] == 2:\n",
        "    test_probs1=tf.nn.sigmoid(tf.convert_to_tensor(model1.predict([TS1, TS2])))\n",
        "elif trainY.shape[1] == 5:\n",
        "    test_probs1=tf.nn.softmax(tf.convert_to_tensor(model1.predict([TS1, TS2])), axis=1)\n",
        "test_preds1=tf.argmax(test_probs1, axis=1)\n",
        "clsCount=2\n",
        "if trainY.shape[1] == 2:\n",
        "    test_probs2=tf.nn.sigmoid(tf.convert_to_tensor(model2.predict([TS1, TS2])))\n",
        "elif trainY.shape[1] == 5:\n",
        "    test_probs2=tf.nn.softmax(tf.convert_to_tensor(model2.predict([TS1, TS2])), axis=1)\n",
        "test_preds2=tf.argmax(test_probs2, axis=1)\n",
        "test_preds1=test_preds1.numpy()\n",
        "test_preds2=test_preds2.numpy()\n",
        "test_pred1_labels=tf.one_hot(tf.convert_to_tensor(test_preds1), depth=trainY.shape[1])\n",
        "test_pred2_labels=tf.one_hot(tf.convert_to_tensor(test_preds2), depth=trainY.shape[1])\n",
        "if trainY.shape[1] == 2:\n",
        "    acc1=tf_keras.metrics.BinaryAccuracy()\n",
        "    acc2=tf_keras.metrics.BinaryAccuracy()\n",
        "elif trainY.shape[1] == 5:\n",
        "    acc1=tf_keras.metrics.CategoricalAccuracy()\n",
        "    acc2=tf_keras.metrics.CategoricalAccuracy()\n",
        "acc1.update_state(testY, test_pred1_labels)\n",
        "acc2.update_state(testY, test_pred2_labels)\n",
        "avg_acc=tf.reduce_mean([acc1.result().numpy(), acc2.result().numpy()])\n",
        "print(avg_acc.numpy())"
      ],
      "metadata": {
        "id": "0JJoT7G5h_2X",
        "execution": {
          "iopub.status.busy": "2024-07-08T01:05:30.573335Z",
          "iopub.execute_input": "2024-07-08T01:05:30.573716Z",
          "iopub.status.idle": "2024-07-08T01:06:28.265375Z",
          "shell.execute_reply.started": "2024-07-08T01:05:30.573687Z",
          "shell.execute_reply": "2024-07-08T01:06:28.264423Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THE SUPERVISED MODEL FOR THE SAME DATA.IF YOU HAVE RAN THE SEMI SUPERVISED MODEL FIRST, THEN YOU HAVE TO RUN FIRST THE CELLS WHICH START WITH THE DEFINITION OF TRAIN_DF['cleaned'] UNTIL THE SEMI SUPERVISED MODEL.THEN RUN THIS CELL\n",
        "config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.num_labels=trainY.shape[1]\n",
        "config.hidden_dropout_prob=0.2\n",
        "modelSup=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "lr=1e-5\n",
        "num_epochs=100\n",
        "if trainY.shape[1] == 2:\n",
        "  lossSup=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "  metricSup=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif trainY.shape[1] == 5:\n",
        "  lossSup=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "  metricSup=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "optSup=tf_keras.optimizers.Adam(learning_rate=lr)\n",
        "callbackSup=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "metricSup=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "modelSup.compile(optimizer=optSup, loss=lossSup, metrics=[metricSup])\n",
        "modelSup.fit([TR1, TR2], trainY, batch_size=32, epochs=num_epochs, shuffle=True, callbacks=[callbackSup], verbose=1, validation_data=([V1, V2], valY))"
      ],
      "metadata": {
        "id": "XZdCSQtBa-eW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = modelSup.evaluate([TS1, TS2], testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "Z1N-IJh2aUv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yw5MxdfkUf6m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}