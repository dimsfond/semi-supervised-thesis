{
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification, BertConfig\n",
        "import tf_keras\n",
        "import pandas as pd\n",
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import plotly.offline as pyo\n",
        "import plotly.graph_objects as go\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import keras\n",
        "from keras.callbacks import EarlyStopping\n",
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import BatchEncoding\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ZTSIVaQPAh4c",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:11:43.721489Z",
          "iopub.execute_input": "2024-07-05T21:11:43.722294Z",
          "iopub.status.idle": "2024-07-05T21:12:14.543054Z",
          "shell.execute_reply.started": "2024-07-05T21:11:43.722262Z",
          "shell.execute_reply": "2024-07-05T21:12:14.542210Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (1)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "FofLrGvxTdbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (2)\n",
        "train_df=pd.read_csv('/content/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
        "test_df=pd.read_csv('/content/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
        "train_df=train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "test_df=test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "train_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "test_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "df=pd.concat([train_df, test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['Sentiment'], random_state=42)\n",
        "mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive':3, 'Extremely Positive':4}\n",
        "train_df['Sentiment']=train_df['Sentiment'].replace(mapping)\n",
        "val_df['Sentiment']=val_df['Sentiment'].replace(mapping)\n",
        "test_df['Sentiment']=test_df['Sentiment'].replace(mapping)\n",
        "train_df['Sentiment']=train_df['Sentiment'].astype('category')\n",
        "val_df['Sentiment']=val_df['Sentiment'].astype('category')\n",
        "test_df['Sentiment']=test_df['Sentiment'].astype('category')\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.42, stratify=train_df['label'], random_state=42)\n",
        "unlab_df['Sentiment']=unlab_df['Sentiment'].cat.add_categories(5)\n",
        "unlab_df['Sentiment']=5\n",
        "#For the first split, put the frac value below to be equal to 0.0051\n",
        "#For the second split, put the frac value below to be equal to 0.0255\n",
        "#For the third split, put the frac value below to be equal to 0.051\n",
        "#For the fourth split, put the frac value below to be equal to 0.2545\n",
        "train_df=train_df.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.2545))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "KC1sKrWqRmxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE IMDB Movie Review DATASET\n",
        "cwd=os.getcwd()\n",
        "data=tf.keras.utils.get_file(fname =\"aclImdb.tar.gz\", origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", cache_dir= cwd, extract = True)\n",
        "data_path=os.path.join(os.path.dirname(data), 'aclImdb')\n",
        "os.listdir(data_path)\n",
        "train_dir=os.path.join(data_path, 'train')\n",
        "for file in os.listdir(train_dir):\n",
        "    file_path=os.path.join(train_dir, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path) as f:\n",
        "            first_line=f.readline().strip()\n",
        "            print(file + \":\" + first_line)\n",
        "def load_dataset(dir, typeOfData):\n",
        "    dictnry = {\"review\": [], \"sentiment\": []}\n",
        "    if typeOfData == 'labelled':\n",
        "        pos_path=os.path.join(dir, 'pos')\n",
        "        neg_path=os.path.join(dir, 'neg')\n",
        "        for file in os.listdir(pos_path):\n",
        "            file_path=os.path.join(pos_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(1)\n",
        "        for file in os.listdir(neg_path):\n",
        "            file_path=os.path.join(neg_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(0)\n",
        "    elif typeOfData == 'unsup':\n",
        "        unsup_path=os.path.join (dir, 'unsup')\n",
        "        for file in os.listdir(unsup_path):\n",
        "            file_path=os.path.join(unsup_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(2)\n",
        "    return dictnry\n",
        "Train_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'labelled'))\n",
        "Unlab_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'unsup'))\n",
        "test_dir=os.path.join(data_path, 'test')\n",
        "Test_df=pd.DataFrame.from_dict(load_dataset(test_dir, 'labelled'))\n",
        "df=pd.concat([Train_df, Test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['sentiment'], random_state=42)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.375, stratify=train_df['sentiment'], random_state=42)\n",
        "unlab_df.loc[:, 'sentiment']=2\n",
        "#For the first split, put the frac value from the train_df below to be equal to 0.004 for about 100 labeled\n",
        "#For the second split, put the frac value from the train_df below to be equal to 0.012 for about 300 labeled\n",
        "#For the third split, put the frac value from the train_df below to be equal to 0.04 for about 1000 labeled\n",
        "#For the fourth split, put the frac value from the train_df below to be equal to 0.2 for about 5000 labeled\n",
        "train_df=train_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n",
        "val_df=val_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "test_df=test_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "NeoqbDdyuxJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (1)\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "train_dataset = sst2_dataset[\"train\"]\n",
        "validation_dataset = sst2_dataset[\"validation\"]\n",
        "test_dataset = sst2_dataset[\"test\"]\n",
        "train_df=pd.DataFrame(train_dataset.to_pandas())\n",
        "val_df=pd.DataFrame(validation_dataset.to_pandas())\n",
        "test_df=pd.DataFrame(test_dataset.to_pandas())\n",
        "df=pd.concat([train_df, val_df], ignore_index=True)\n",
        "df.drop(columns=['idx'], inplace=True)\n",
        "print(df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T21:12:17.775152Z",
          "iopub.execute_input": "2024-07-05T21:12:17.776438Z",
          "iopub.status.idle": "2024-07-05T21:12:21.725558Z",
          "shell.execute_reply.started": "2024-07-05T21:12:17.776402Z",
          "shell.execute_reply": "2024-07-05T21:12:21.724554Z"
        },
        "trusted": true,
        "id": "X579VuDawO4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (2)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['label'], random_state=42)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.4, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=2\n",
        "print(train_df)\n",
        "print(unlab_df)\n",
        "print(val_df)\n",
        "print(test_df)\n",
        "#For the first split, put the frac value below to be equal to 0.0031 for about 100 labeled samples\n",
        "#For the second split, put the frac value below to be equal to 0.01 for about 300 labeled samples\n",
        "#For the third split, put the frac value below to be equal to 0.0153 for about 500 labeled samples\n",
        "#For the fourth split, put the frac value below to be equal to 0.031 for about 1000 labeled samples\n",
        "#For the fifth split put the frac value below to be equal to 0.0765 for about 2500 labeled samples\n",
        "#For the sixth split, put the frac value below to be equal to 0.153 for about 5000 labeled samples\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.153))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-05T21:12:27.726696Z",
          "iopub.execute_input": "2024-07-05T21:12:27.727408Z",
          "iopub.status.idle": "2024-07-05T21:12:27.817537Z",
          "shell.execute_reply.started": "2024-07-05T21:12:27.727378Z",
          "shell.execute_reply": "2024-07-05T21:12:27.816659Z"
        },
        "trusted": true,
        "id": "L0vJx9iAwO4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT SST-5 DATASET\n",
        "dataset=load_dataset(\"SetFit/sst5\")\n",
        "train_dataset=dataset['train']\n",
        "val_dataset=dataset['validation']\n",
        "test_dataset=dataset['test']\n",
        "train_df=pd.DataFrame(train_dataset)\n",
        "train_df=train_df.drop(columns=['label_text'])\n",
        "val_df=pd.DataFrame(val_dataset)\n",
        "val_df=val_df.drop(columns=['label_text'])\n",
        "test_df=pd.DataFrame(test_dataset)\n",
        "test_df=test_df.drop(columns=['label_text'])\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.53, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=5\n",
        "#For the first split, put the frac value below to be equal to 0.027\n",
        "#For the second split, put the frac value below to be equal to 0.125\n",
        "#For the third split, put the frac value below to be equal to 0.25\n",
        "#For the fourth split, put the frac value below to be equal to 0.5\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.5))\n",
        "train_df.drop_duplicates(subset='text',inplace=True)\n",
        "unlab_df.drop_duplicates(subset='text', inplace=True)\n",
        "val_df.drop_duplicates(subset='text',inplace=True)\n",
        "test_df.drop_duplicates(subset='text',inplace=True)\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "w1Fwah6XMEi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    soup=BeautifulSoup(text, \"html.parser\")\n",
        "    text=re.sub(r\"\\[[^]]*\\]\" , '', soup.get_text())\n",
        "    text=re.sub(r\"[^A-Za-z0-9\\s,']\", '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Sueytj9zV6zV",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:12.899667Z",
          "iopub.execute_input": "2024-07-05T21:34:12.900401Z",
          "iopub.status.idle": "2024-07-05T21:34:12.905674Z",
          "shell.execute_reply.started": "2024-07-05T21:34:12.900368Z",
          "shell.execute_reply": "2024-07-05T21:34:12.904551Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['cleaned']=train_df[train_df.columns[0]].apply(clean_text)\n",
        "unlab_df['cleaned']=unlab_df[train_df.columns[0]].apply(clean_text)\n",
        "val_df['cleaned']=val_df[train_df.columns[0]].apply(clean_text)\n",
        "test_df['cleaned']=test_df[train_df.columns[0]].apply(clean_text)"
      ],
      "metadata": {
        "id": "E1Rlaa6Xa0ER",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:13.801826Z",
          "iopub.execute_input": "2024-07-05T21:34:13.802223Z",
          "iopub.status.idle": "2024-07-05T21:34:15.862752Z",
          "shell.execute_reply.started": "2024-07-05T21:34:13.802196Z",
          "shell.execute_reply": "2024-07-05T21:34:15.861741Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_review=train_df['cleaned']\n",
        "unlab_review=unlab_df['cleaned']\n",
        "valX=val_df['cleaned']\n",
        "testX=test_df['cleaned']\n",
        "train_target=train_df[train_df.columns[1]]\n",
        "valY=val_df[train_df.columns[1]]\n",
        "testY=test_df[train_df.columns[1]]\n",
        "unsup_target=unlab_df[train_df.columns[1]]"
      ],
      "metadata": {
        "id": "gvPslA4NfGK-",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:17.116552Z",
          "iopub.execute_input": "2024-07-05T21:34:17.117390Z",
          "iopub.status.idle": "2024-07-05T21:34:17.125416Z",
          "shell.execute_reply.started": "2024-07-05T21:34:17.117357Z",
          "shell.execute_reply": "2024-07-05T21:34:17.124491Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tz=BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "3hBS2hq7JpqB",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:18.279723Z",
          "iopub.execute_input": "2024-07-05T21:34:18.280093Z",
          "iopub.status.idle": "2024-07-05T21:34:18.426462Z",
          "shell.execute_reply.started": "2024-07-05T21:34:18.280064Z",
          "shell.execute_reply": "2024-07-05T21:34:18.425679Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len= 64\n",
        "X_train_encoded = tz.batch_encode_plus(train_review.tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')\n",
        "X_val_encoded = tz.batch_encode_plus(valX.tolist(),\n",
        "                                              padding='max_length',\n",
        "                                              truncation=True, max_length=max_len,\n",
        "                                              return_tensors='tf')\n",
        "X_test_encoded = tz.batch_encode_plus(testX.tolist(),\n",
        "                                              padding='max_length',\n",
        "                                              truncation=True, max_length=max_len,\n",
        "                                              return_tensors='tf')\n",
        "X_unsup_encoded=tz.batch_encode_plus(unlab_review.tolist(), padding='max_length', truncation=True, max_length=max_len, return_tensors='tf')"
      ],
      "metadata": {
        "id": "jsR_5fKwo3Ud",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:19.287381Z",
          "iopub.execute_input": "2024-07-05T21:34:19.287747Z",
          "iopub.status.idle": "2024-07-05T21:34:37.117619Z",
          "shell.execute_reply.started": "2024-07-05T21:34:19.287719Z",
          "shell.execute_reply": "2024-07-05T21:34:37.116799Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TR1=X_train_encoded['input_ids']\n",
        "TR2=X_train_encoded['attention_mask']\n",
        "U1=X_unsup_encoded['input_ids']\n",
        "U2=X_unsup_encoded['attention_mask']\n",
        "V1=X_val_encoded['input_ids']\n",
        "V2=X_val_encoded['attention_mask']\n",
        "TS1=X_test_encoded['input_ids']\n",
        "TS2=X_test_encoded['attention_mask']"
      ],
      "metadata": {
        "id": "jnqhpSf9mDk6",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:38.859882Z",
          "iopub.execute_input": "2024-07-05T21:34:38.860245Z",
          "iopub.status.idle": "2024-07-05T21:34:38.866115Z",
          "shell.execute_reply.started": "2024-07-05T21:34:38.860217Z",
          "shell.execute_reply": "2024-07-05T21:34:38.865198Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(TR1.shape)\n",
        "print(TR2.shape)\n",
        "print(V1.shape)\n",
        "print(V2.shape)\n",
        "print(TS1.shape)\n",
        "print(TS2.shape)\n",
        "print(U1.shape)\n",
        "print(U2.shape)\n",
        "print(train_target.shape)\n",
        "print(valY.shape)\n",
        "print(testY.shape)\n",
        "print(unsup_target.shape)"
      ],
      "metadata": {
        "id": "CoRMkBgx0ruq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY=tf.one_hot(train_target, depth=train_target.nunique())\n",
        "valY=tf.one_hot(valY, depth=valY.nunique())\n",
        "testY=tf.one_hot(testY, depth=testY.nunique())"
      ],
      "metadata": {
        "id": "0qnwStkArt6b",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:40.835432Z",
          "iopub.execute_input": "2024-07-05T21:34:40.836177Z",
          "iopub.status.idle": "2024-07-05T21:34:40.843721Z",
          "shell.execute_reply.started": "2024-07-05T21:34:40.836144Z",
          "shell.execute_reply": "2024-07-05T21:34:40.842892Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j=0 # the current number of iteration\n",
        "iter=2 #the number of iterations for which the first dropout value will be applied, you change it according to the experiments in the pdf. If the dropout value is the same in all iterations, you just put the sae value twice below wehre you set dropout\n",
        "total=0 #the sum of all iterations\n",
        "history=[]\n",
        "while U1.shape[0] != 0:\n",
        "    config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "    config.num_labels=trainY.shape[1]\n",
        "    if j < iter:\n",
        "        config.hidden_dropout_prob=0.2\n",
        "    elif j >= iter:\n",
        "        config.hidden_dropout_prob=0.2\n",
        "    model=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "    num_epochs=100\n",
        "    if trainY.shape[1] == 2:\n",
        "        loss=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "        metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "    elif trainY.shape[1] == 5:\n",
        "        loss=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "    if j == 0:\n",
        "        initial_lr=1e-5\n",
        "        optimizer=tf_keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "        callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "    if j > 0:\n",
        "        lr=9e-6\n",
        "        optimizer=tf_keras.optimizers.Adam(learning_rate=lr)\n",
        "        callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "    history.append(model.fit([TR1, TR2], trainY, validation_data=([V1, V2], valY), batch_size=32, epochs=num_epochs, shuffle=True, callbacks=[callback], verbose=1))\n",
        "    y_pred=model.predict([U1, U2])\n",
        "    if trainY.shape[1] == 2:\n",
        "        probs=tf.reduce_max(tf.nn.sigmoid(y_pred['logits']), axis=1)\n",
        "        preds=tf.one_hot(tf.argmax(tf.nn.sigmoid(y_pred['logits']), axis=1), depth=trainY.shape[1])\n",
        "    elif trainY.shape[1] == 5:\n",
        "        probs=tf.reduce_max(tf.nn.softmax(y_pred['logits'], axis=1), axis=1)\n",
        "        preds=tf.one_hot(tf.argmax(tf.nn.softmax(y_pred['logits'], axis=1), axis=1), depth=trainY.shape[1])\n",
        "    count=0\n",
        "    indexes=[]\n",
        "    p=0.9 #p, the threshold which the max predicted probability of a sample must overcome so that it be added to the training data\n",
        "    for i in range(probs.shape[0]):\n",
        "        if probs[i] >= p:\n",
        "            indexes.append(i)\n",
        "            count+=1\n",
        "            TR1=tf.concat([TR1, tf.reshape(U1[i], (1, max_len))], axis=0)\n",
        "            TR2=tf.concat([TR2, tf.reshape(U2[i], (1, max_len))], axis=0)\n",
        "            trainY=tf.concat([trainY, tf.reshape(preds[i], (1, trainY.shape[1]))], axis=0)\n",
        "    print(TR1.shape)\n",
        "    print(TR2.shape)\n",
        "    print(trainY.shape)\n",
        "    if len(indexes) != 0:\n",
        "        maskX=tf.ones(U1.shape[0], tf.bool)\n",
        "        maskY=tf.ones(unsup_target.shape[0], tf.bool)\n",
        "        indices_to_update=tf.expand_dims(indexes, axis=1)\n",
        "        updatesX=tf.zeros_like(indexes, dtype=tf.bool)\n",
        "        updatesY=tf.zeros_like(indexes, dtype=tf.bool)\n",
        "        maskX=tf.tensor_scatter_nd_update(maskX, indices_to_update, updatesX)\n",
        "        maskY=tf.tensor_scatter_nd_update(maskY, indices_to_update, updatesY)\n",
        "        U1=tf.boolean_mask(U1, maskX)\n",
        "        U2=tf.boolean_mask(U2, maskX)\n",
        "        unsup_target=tf.boolean_mask(unsup_target, maskY)\n",
        "        print(U1.shape)\n",
        "        print(U2.shape)\n",
        "        print(unsup_target.shape)\n",
        "    print(count, \"samples were added to the labelled set\")\n",
        "    total+=count\n",
        "    j+=1\n",
        "    if j==4:\n",
        "        break"
      ],
      "metadata": {
        "id": "ZDyIX5fo2_qT",
        "execution": {
          "iopub.status.busy": "2024-07-05T21:34:42.032877Z",
          "iopub.execute_input": "2024-07-05T21:34:42.033610Z",
          "iopub.status.idle": "2024-07-05T22:52:21.411956Z",
          "shell.execute_reply.started": "2024-07-05T21:34:42.033579Z",
          "shell.execute_reply": "2024-07-05T22:52:21.410898Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate([TS1, TS2], testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "EDK9aRV64FYR",
        "execution": {
          "iopub.status.busy": "2024-07-05T23:02:16.778435Z",
          "iopub.execute_input": "2024-07-05T23:02:16.778818Z",
          "iopub.status.idle": "2024-07-05T23:02:46.854740Z",
          "shell.execute_reply.started": "2024-07-05T23:02:16.778787Z",
          "shell.execute_reply": "2024-07-05T23:02:46.853819Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#HERE YOU RUN THE SUPERVISED MODEL FOR THE SAME DATA. IF YOU HAVE RAN THE SEMI SUPERVISED MODEL FIRST, THEN YOU HAVE TO RUN FIRST THE CELLS WHICH START WITH THE DEFINITION OF TRAIN REVIEW, TRAIN TARGET ETC UNTIL THE SEMI SUPERVISED MODEL.THEN RUN THIS CELL\n",
        "num_epochs=100\n",
        "config=BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.num_labels=trainY.shape[1]\n",
        "config.hidden_dropout_prob=0.2\n",
        "model=TFBertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
        "initial_lr=1e-5\n",
        "optimizer=tf_keras.optimizers.Adam(learning_rate=initial_lr)\n",
        "if trainY.shape[1] == 2:\n",
        "    loss=tf_keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif trainY.shape[1] == 5:\n",
        "    loss=tf_keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "historySup=model.fit([TR1, TR2], trainY, validation_data=([V1, V2], valY), batch_size=32, epochs=num_epochs, shuffle=True, callbacks=[callback], verbose=1)"
      ],
      "metadata": {
        "id": "NZV_1eVc1sVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate([TS1, TS2], testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "1bbBTY7Aj7az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ChtKSE4g823"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}