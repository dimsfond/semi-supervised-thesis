{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 7564750,
          "sourceType": "datasetVersion",
          "datasetId": 4404752
        },
        {
          "sourceId": 7825245,
          "sourceType": "datasetVersion",
          "datasetId": 4585370
        },
        {
          "sourceId": 8001943,
          "sourceType": "datasetVersion",
          "datasetId": 4712261
        }
      ],
      "dockerImageVersionId": 30699,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "import keras\n",
        "import tf_keras\n",
        "from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification, BertConfig\n",
        "from tf_keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization\n",
        "from tf_keras.models import Model\n",
        "from tf_keras.optimizers import Adam\n",
        "import math\n",
        "import tensorflow_hub as hub\n",
        "from sklearn.utils import resample\n",
        "tf.config.run_functions_eagerly(True)\n",
        "!pip install datasets\n",
        "import datasets\n",
        "from datasets import load_dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "iNnzDd0fVtvQ",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:05:10.241328Z",
          "iopub.execute_input": "2024-06-30T13:05:10.241709Z",
          "iopub.status.idle": "2024-06-30T13:05:42.124688Z",
          "shell.execute_reply.started": "2024-06-30T13:05:10.241675Z",
          "shell.execute_reply": "2024-06-30T13:05:42.123585Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (1)\n",
        "sst2_dataset = load_dataset(\"glue\", \"sst2\")\n",
        "train_dataset = sst2_dataset[\"train\"]\n",
        "validation_dataset = sst2_dataset[\"validation\"]\n",
        "test_dataset = sst2_dataset[\"test\"]\n",
        "train_df=pd.DataFrame(train_dataset.to_pandas())\n",
        "val_df=pd.DataFrame(validation_dataset.to_pandas())\n",
        "test_df=pd.DataFrame(test_dataset.to_pandas())\n",
        "df=pd.concat([train_df, val_df], ignore_index=True)\n",
        "df.drop(columns=['idx'], inplace=True)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "oprRnzK82nzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE SST-2 DATASET (2)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['label'], random_state=42)\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "val_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.4, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=2\n",
        "print(train_df)\n",
        "print(unlab_df)\n",
        "print(val_df)\n",
        "print(test_df)\n",
        "#For the first split, put the frac value below to be equal to 0.0031 for about 100 labeled samples\n",
        "#For the second split, put the frac value below to be equal to 0.01 for about 300 labeled samples\n",
        "#For the third split, put the frac value below to be equal to 0.0153 for about 500 labeled samples\n",
        "#For the fourth split, put the frac value below to be equal to 0.031 for about 1000 labeled samples\n",
        "#For the fifth split put the frac value below to be equal to 0.0765 for about 2500 labeled samples\n",
        "#For the sixth split, put the frac value below to be equal to 0.153 for about 5000 labeled samples\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.153))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "FFYt2t7Y2oe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE IMDB Movie Review DATASET\n",
        "cwd=os.getcwd()\n",
        "data=tf.keras.utils.get_file(fname =\"aclImdb.tar.gz\", origin=\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", cache_dir= cwd, extract = True)\n",
        "data_path=os.path.join(os.path.dirname(data), 'aclImdb')\n",
        "os.listdir(data_path)\n",
        "train_dir=os.path.join(data_path, 'train')\n",
        "for file in os.listdir(train_dir):\n",
        "    file_path=os.path.join(train_dir, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        with open(file_path) as f:\n",
        "            first_line=f.readline().strip()\n",
        "            print(file + \":\" + first_line)\n",
        "def load_dataset(dir, typeOfData):\n",
        "    dictnry = {\"review\": [], \"sentiment\": []}\n",
        "    if typeOfData == 'labelled':\n",
        "        pos_path=os.path.join(dir, 'pos')\n",
        "        neg_path=os.path.join(dir, 'neg')\n",
        "        for file in os.listdir(pos_path):\n",
        "            file_path=os.path.join(pos_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(1)\n",
        "        for file in os.listdir(neg_path):\n",
        "            file_path=os.path.join(neg_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(0)\n",
        "    elif typeOfData == 'unsup':\n",
        "        unsup_path=os.path.join (dir, 'unsup')\n",
        "        for file in os.listdir(unsup_path):\n",
        "            file_path=os.path.join(unsup_path, file)\n",
        "            with open(file_path, 'r') as f:\n",
        "                dictnry[\"review\"].append(f.read())\n",
        "                dictnry[\"sentiment\"].append(2)\n",
        "    return dictnry\n",
        "Train_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'labelled'))\n",
        "Unlab_df=pd.DataFrame.from_dict(load_dataset(train_dir, 'unsup'))\n",
        "test_dir=os.path.join(data_path, 'test')\n",
        "Test_df=pd.DataFrame.from_dict(load_dataset(test_dir, 'labelled'))\n",
        "df=pd.concat([Train_df, Test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['sentiment'], random_state=42)\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.375, stratify=train_df['sentiment'], random_state=42)\n",
        "unlab_df.loc[:, 'sentiment']=2\n",
        "#For the first split, put the frac value from the train_df below to be equal to 0.004 for about 100 labeled\n",
        "#For the second split, put the frac value from the train_df below to be equal to 0.012 for about 300 labeled\n",
        "#For the third split, put the frac value from the train_df below to be equal to 0.04 for about 1000 labeled\n",
        "#For the fourth split, put the frac value from the train_df below to be equal to 0.2 for about 5000 labeled\n",
        "train_df=train_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.2))\n",
        "val_df=val_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "test_df=test_df.groupby('sentiment', group_keys=False).apply(lambda x: x.sample(frac=1))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "cfdWDzx32tm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (1)\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "LrOqM2f42fRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT THE Coronavirus NLP tweets DATASET (2)\n",
        "train_df=pd.read_csv('/content/Corona_NLP_train.csv', encoding='ISO-8859-1')\n",
        "test_df=pd.read_csv('/content/Corona_NLP_test.csv', encoding='ISO-8859-1')\n",
        "train_df=train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "test_df=test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\n",
        "train_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "test_df.drop_duplicates(subset='OriginalTweet',inplace=True)\n",
        "df=pd.concat([train_df, test_df], axis=0)\n",
        "train_df, test_df=train_test_split(df, test_size=0.2, stratify=df['Sentiment'], random_state=42)\n",
        "val_df, test_df=train_test_split(test_df, test_size=0.5, stratify=test_df['Sentiment'], random_state=42)\n",
        "mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive':3, 'Extremely Positive':4}\n",
        "train_df['Sentiment']=train_df['Sentiment'].replace(mapping)\n",
        "val_df['Sentiment']=val_df['Sentiment'].replace(mapping)\n",
        "test_df['Sentiment']=test_df['Sentiment'].replace(mapping)\n",
        "train_df['Sentiment']=train_df['Sentiment'].astype('category')\n",
        "val_df['Sentiment']=val_df['Sentiment'].astype('category')\n",
        "test_df['Sentiment']=test_df['Sentiment'].astype('category')\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.42, stratify=train_df['label'], random_state=42)\n",
        "unlab_df['Sentiment']=unlab_df['Sentiment'].cat.add_categories(5)\n",
        "unlab_df['Sentiment']=5\n",
        "#For the first split, put the frac value below to be equal to 0.0051\n",
        "#For the second split, put the frac value below to be equal to 0.0255\n",
        "#For the third split, put the frac value below to be equal to 0.051\n",
        "#For the fourth split, put the frac value below to be equal to 0.2545\n",
        "train_df=train_df.groupby('Sentiment', group_keys=False).apply(lambda x: x.sample(frac=0.0051))\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "id": "Rpq3tnzR2fRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RUN ONLY IF YOU WANT SST-5 DATASET\n",
        "dataset=load_dataset(\"SetFit/sst5\")\n",
        "train_dataset=dataset['train']\n",
        "val_dataset=dataset['validation']\n",
        "test_dataset=dataset['test']\n",
        "train_df=pd.DataFrame(train_dataset)\n",
        "train_df=train_df.drop(columns=['label_text'])\n",
        "val_df=pd.DataFrame(val_dataset)\n",
        "val_df=val_df.drop(columns=['label_text'])\n",
        "test_df=pd.DataFrame(test_dataset)\n",
        "test_df=test_df.drop(columns=['label_text'])\n",
        "train_df, unlab_df= train_test_split(train_df, test_size=0.53, stratify=train_df['label'], random_state=42)\n",
        "unlab_df.loc[:, 'label']=5\n",
        "#For the first split, put the frac value below to be equal to 0.027\n",
        "#For the second split, put the frac value below to be equal to 0.125\n",
        "#For the third split, put the frac value below to be equal to 0.25\n",
        "#For the fourth split, put the frac value below to be equal to 0.5\n",
        "train_df=train_df.groupby('label', group_keys=False).apply(lambda x: x.sample(frac=0.125))\n",
        "train_df.drop_duplicates(subset='text',inplace=True)\n",
        "unlab_df.drop_duplicates(subset='text', inplace=True)\n",
        "val_df.drop_duplicates(subset='text',inplace=True)\n",
        "test_df.drop_duplicates(subset='text',inplace=True)\n",
        "print(len(train_df))\n",
        "print(len(unlab_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-06-29T22:08:23.026981Z",
          "iopub.execute_input": "2024-06-29T22:08:23.027404Z",
          "iopub.status.idle": "2024-06-29T22:08:27.562906Z",
          "shell.execute_reply.started": "2024-06-29T22:08:23.027359Z",
          "shell.execute_reply": "2024-06-29T22:08:27.561655Z"
        },
        "trusted": true,
        "id": "ChfmoZE52fRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    soup=BeautifulSoup(text, \"html.parser\")\n",
        "    text=re.sub(r\"\\[[^]]*\\]\" , '', soup.get_text())\n",
        "    no_url=re.sub(r'http\\S+','', text)\n",
        "    lowerCase=no_url.lower()\n",
        "    new_text=re.sub(r\"[^a-zA-Z\\s,'#]\", '', lowerCase)\n",
        "    new_text=re.sub(r'[\\r\\n\\t]', '', new_text)\n",
        "    new_text=re.sub(r'#\\S*', '', new_text)\n",
        "    return new_text"
      ],
      "metadata": {
        "id": "m6MIh9RkVtvU",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:06:31.898981Z",
          "iopub.execute_input": "2024-06-30T13:06:31.899346Z",
          "iopub.status.idle": "2024-06-30T13:06:31.905281Z",
          "shell.execute_reply.started": "2024-06-30T13:06:31.899317Z",
          "shell.execute_reply": "2024-06-30T13:06:31.904195Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['Cleaned']=train_df[train_df.columns[0]].apply(clean_text)\n",
        "unlab_df['Cleaned']=unlab_df[train_df.columns[0]].apply(clean_text)\n",
        "val_df['Cleaned']=val_df[train_df.columns[0]].apply(clean_text)\n",
        "test_df['Cleaned']=test_df[train_df.columns[0]].apply(clean_text)"
      ],
      "metadata": {
        "id": "1ozv5FxXVtvW",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:06:33.126756Z",
          "iopub.execute_input": "2024-06-30T13:06:33.127134Z",
          "iopub.status.idle": "2024-06-30T13:06:35.287423Z",
          "shell.execute_reply.started": "2024-06-30T13:06:33.127099Z",
          "shell.execute_reply": "2024-06-30T13:06:35.286639Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX=train_df['Cleaned']\n",
        "trainY=train_df[train_df.columns[1]]\n",
        "unlabX=unlab_df['Cleaned']\n",
        "unlabY=unlab_df[train_df.columns[1]]\n",
        "valX=val_df['Cleaned']\n",
        "valY=val_df[train_df.columns[1]]\n",
        "testX=test_df['Cleaned']\n",
        "testY=test_df[train_df.columns[1]]\n",
        "num_classes=len(trainY.unique())"
      ],
      "metadata": {
        "id": "9tL-1A8aVtvZ",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:06:36.853062Z",
          "iopub.execute_input": "2024-06-30T13:06:36.853705Z",
          "iopub.status.idle": "2024-06-30T13:06:36.861421Z",
          "shell.execute_reply.started": "2024-06-30T13:06:36.853672Z",
          "shell.execute_reply": "2024-06-30T13:06:36.860367Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPGenerator(tf.keras.Model):\n",
        "    def __init__(self, layer_size=768, output_size=768):\n",
        "        super().__init__()\n",
        "        self.dense1=Dense(units=layer_size)\n",
        "        self.activation1=tf_keras.layers.LeakyReLU()\n",
        "        self.drop1=Dropout(0.1)\n",
        "        self.output_layer=tf_keras.layers.Dense(units=output_size, activation=None)\n",
        "    def call(self, input, training=True):\n",
        "        x=self.dense1(input)\n",
        "        x=self.activation1(x)\n",
        "        x=self.drop1(x, training=training)\n",
        "        self.x=self.output_layer(x)\n",
        "        return self.x\n",
        "    def gen_loss(self, real_activations, fake_activations, d_fake_outputs, smooth=0.1):\n",
        "        feature_matching=tf.reduce_mean(tf.square(tf.reduce_mean(real_activations, axis=0) - tf.reduce_mean(fake_activations, axis=0)))\n",
        "        gUnsupLoss=-tf.reduce_mean(tf.math.log(1-d_fake_outputs[:,-1]+1e-10))\n",
        "        self.genLoss=feature_matching + gUnsupLoss\n",
        "        return self.genLoss"
      ],
      "metadata": {
        "id": "nVouhcVkVtva",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:06:59.797635Z",
          "iopub.execute_input": "2024-06-30T13:06:59.798247Z",
          "iopub.status.idle": "2024-06-30T13:06:59.811059Z",
          "shell.execute_reply.started": "2024-06-30T13:06:59.798205Z",
          "shell.execute_reply": "2024-06-30T13:06:59.809121Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPDiscriminator(tf.keras.Model):\n",
        "    def __init__(self, layer_size=768, output_size=num_classes+1):\n",
        "        super().__init__()\n",
        "        self.bert_model=TFBertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dense1=tf_keras.layers.Dense(units=layer_size)\n",
        "        self.activation1=tf_keras.layers.LeakyReLU()\n",
        "        self.drop1=Dropout(0.1)\n",
        "        self.dense2=tf_keras.layers.Dense(units=output_size, activation=None)\n",
        "    def call(self, inputs, training=True):\n",
        "        if dataType == 'R':\n",
        "            inputIds, attentionMask= inputs\n",
        "            embeddings=self.bert_model(inputIds, attentionMask)['last_hidden_state'][:,0]\n",
        "        elif dataType == 'F':\n",
        "            embeddings=inputs\n",
        "        x=self.dense1(embeddings)\n",
        "        self.activations=self.activation1(x)\n",
        "        x=self.drop1(x, training=training)\n",
        "        self.logitsD=self.dense2(x)\n",
        "        self.d_output=tf.nn.softmax(self.logitsD, axis=-1)\n",
        "        return self.activations, self.logitsD, self.d_output\n",
        "\n",
        "    def disc_loss(self, d_real_logits, d_real_outputs, targets, training, d_fake_outputs=None, label_mask=None):\n",
        "        if training:\n",
        "            if tf.reduce_any(tf.not_equal(targets, num_classes)).numpy() == 0:\n",
        "                disc_supLoss=0\n",
        "            else:\n",
        "                masked_real_logits=tf.boolean_mask(d_real_logits, label_mask)\n",
        "                if num_classes == 2:\n",
        "                    real_probs=tf.nn.sigmoid(masked_real_logits[:,0:-1])\n",
        "                    log_probs=tf.math.log(real_probs)\n",
        "                elif num_classes == 5:\n",
        "                    log_probs=tf.nn.log_softmax(masked_real_logits[:,0:-1])\n",
        "                masked_targets=tf.boolean_mask(targets, label_mask)\n",
        "                one_hot_labels=tf.one_hot(masked_targets, num_classes)\n",
        "                per_data_sup_loss=-tf.reduce_sum(one_hot_labels*log_probs)\n",
        "                disc_supLoss=tf.divide(per_data_sup_loss, tf.maximum(1.0, tf.cast(log_probs.shape[0], tf.float32)))\n",
        "            unlab_mask=tf.zeros(d_real_outputs.shape[0])\n",
        "            for i in range(d_real_outputs.shape[0]):\n",
        "                if targets[i] != num_classes and tf.argmax(d_real_outputs[i]) == num_classes:\n",
        "                    unlab_mask=tf.tensor_scatter_nd_update(unlab_mask, [[i]], [1])\n",
        "            masked_real_outputs=tf.boolean_mask(d_real_outputs, unlab_mask)\n",
        "            if masked_real_outputs.shape[0] == 0:\n",
        "                real_unsup_loss=0.0\n",
        "            else:\n",
        "                real_unsup_loss=-tf.reduce_mean(tf.math.log(1-masked_real_outputs[:,-1])+1e-10)\n",
        "            fake_unsup_loss=-tf.reduce_mean(tf.math.log(1-d_fake_outputs[:,-1])+1e-10)\n",
        "            disc_unsupLoss=real_unsup_loss+fake_unsup_loss\n",
        "            discLoss=disc_supLoss+disc_unsupLoss\n",
        "            return discLoss\n",
        "        else:\n",
        "            one_hot_labels=tf.one_hot(targets, num_classes)\n",
        "            if num_classes == 2:\n",
        "                real_probs=tf.nn.sigmoid(d_real_logits[:,0:-1])\n",
        "                log_probs=tf.math.log(real_probs)\n",
        "            elif num_classes == 5:\n",
        "                log_probs=tf.nn.log_softmax(d_real_logits[:,0:-1])\n",
        "            per_data_sup_loss=-tf.reduce_sum(one_hot_labels*log_probs)\n",
        "            disc_supLoss=tf.reduce_mean(per_data_sup_loss)\n",
        "            discLoss=disc_supLoss\n",
        "            return discLoss"
      ],
      "metadata": {
        "id": "zdQqBqZ2Vtvb",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:04.381936Z",
          "iopub.execute_input": "2024-06-30T13:07:04.382290Z",
          "iopub.status.idle": "2024-06-30T13:07:04.401232Z",
          "shell.execute_reply.started": "2024-06-30T13:07:04.382260Z",
          "shell.execute_reply": "2024-06-30T13:07:04.400060Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANBERT(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.gen=MLPGenerator()\n",
        "        self.disc=MLPDiscriminator()\n",
        "        self.tz=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased', num_attention_heads=12)\n",
        "        self.previous_supervised_accuracy=0.0\n",
        "        self.previous_unsupervised_accuracy=0.0\n",
        "\n",
        "    def encode_text(self, input_text, max_length=64):\n",
        "        self.max_length=max_length\n",
        "        tokens=self.tz.batch_encode_plus(input_text.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "        return tokens\n",
        "\n",
        "    def preparing_dataset(self, trainX, trainY, unlabX, unlabY, valX, valY, testX, testY, log_ratio=int(math.log(100, 10))):\n",
        "        self.trainX=trainX\n",
        "        self.trainY=trainY\n",
        "        self.unlabX=unlabX\n",
        "        self.unlabY=unlabY\n",
        "        self.valX=valX\n",
        "        self.valY=valY\n",
        "        self.testX=testX\n",
        "        self.testY=testY\n",
        "        self.trainX_embeddings=self.encode_text(self.trainX)\n",
        "        print(self.trainX_embeddings['input_ids'].shape)\n",
        "        with tf.device('/CPU:0'):\n",
        "            self.trainX_embeddings['input_ids']=tf.repeat(self.trainX_embeddings['input_ids'], repeats=log_ratio, axis=0)\n",
        "            self.trainX_embeddings['attention_mask']=tf.repeat(self.trainX_embeddings['attention_mask'], repeats=log_ratio, axis=0)\n",
        "            self.trainY=tf.repeat(self.trainY, repeats=log_ratio, axis=0)\n",
        "        self.trainX_embeddings['input_ids']=self.trainX_embeddings['input_ids']\n",
        "        print(self.trainX_embeddings['input_ids'].shape)\n",
        "        self.trainX_embeddings['attention_mask']=self.trainX_embeddings['attention_mask']\n",
        "        self.trainY=self.trainY\n",
        "        self.unlabX_embeddings=self.encode_text(self.unlabX)\n",
        "        with tf.device('/CPU:0'):\n",
        "            bertX_inputIds=tf.concat([self.trainX_embeddings['input_ids'], self.unlabX_embeddings['input_ids']], axis=0)\n",
        "            bertX_attentionMask=tf.concat([self.trainX_embeddings['attention_mask'], self.unlabX_embeddings['attention_mask']], axis=0)\n",
        "            self.berty=tf.concat([self.trainY, self.unlabY.to_numpy()], axis=0)\n",
        "        print(bertX_inputIds.shape)\n",
        "        self.berty=self.berty\n",
        "        self.berty=tf.cast(self.berty, tf.int32)\n",
        "        self.valX=self.encode_text(valX)\n",
        "        self.testX=self.encode_text(testX)\n",
        "        num_samples=bertX_inputIds.shape[0]\n",
        "        random_indexes=tf.random.shuffle(tf.range(num_samples))\n",
        "        with tf.device('/CPU:0'):\n",
        "            bertX_inputIds=tf.gather(bertX_inputIds, random_indexes)\n",
        "            bertX_attentionMask=tf.gather(bertX_attentionMask, random_indexes)\n",
        "            self.berty=tf.gather(self.berty, random_indexes)\n",
        "        self.bertX=(bertX_inputIds, bertX_attentionMask)\n",
        "        print(self.bertX[0].shape)\n",
        "        self.berty=self.berty\n",
        "        self.valX=(self.valX['input_ids'], self.valX['attention_mask'])\n",
        "        self.valY=tf.convert_to_tensor(self.valY)\n",
        "        self.valY=self.valY\n",
        "        self.testX=(self.testX['input_ids'], self.testX['attention_mask'])\n",
        "        self.testY=tf.convert_to_tensor(self.testY)\n",
        "        self.testY=self.testY\n",
        "        return self.bertX, self.berty, self.valX, self.valY, self.testX, self.testY\n",
        "\n",
        "    def call(self, real_data, training=True):\n",
        "        global dataType\n",
        "        if training == True:\n",
        "            self.gout=self.gen(self.fake_batch)\n",
        "            dataType='R'\n",
        "            self.real_activs, self.real_logits, self.real_outputs=self.disc(real_data, training=training)\n",
        "            dataType='F'\n",
        "            self.fake_activs, self.fake_logits, self.fake_outputs=self.disc(self.gout, training=training)\n",
        "            return self.gout, self.real_activs, self.real_logits, self.real_outputs, self.fake_activs, self.fake_logits, self.fake_outputs\n",
        "        else:\n",
        "            dataType='R'\n",
        "            test_activs, test_logits, test_outputs=self.disc(real_data, training=training)\n",
        "            return test_activs, test_logits, test_outputs\n",
        "\n",
        "    def compile(self, gen_opt, disc_opt, sup_acc, unsup_acc, run_eagerly=True):\n",
        "        super().compile()\n",
        "        self.gen_opt=gen_opt\n",
        "        self.disc_opt=disc_opt\n",
        "        self.sup_acc=sup_acc\n",
        "        self.unsup_acc=unsup_acc\n",
        "        return self.gen_opt, self.disc_opt, self.sup_acc, self.unsup_acc\n",
        "\n",
        "    def train_step(self, data):\n",
        "        real_batch, real_targets=data\n",
        "        fake_batch_size=32 #YOU SET THE FAKE BATCH SIZE\n",
        "        self.fake_batch=tf.random.normal(mean=0, stddev=1, shape=(fake_batch_size, 100))\n",
        "        self.fake_targets=tf.constant([2]*fake_batch_size)\n",
        "        label_mask=tf.zeros(real_batch[0].shape[0])\n",
        "        count=0\n",
        "        real_unlabeled_idx=[]\n",
        "        for j in real_targets:\n",
        "            if j != num_classes:\n",
        "                label_mask=tf.tensor_scatter_nd_update(label_mask, [[count]], [1])\n",
        "            else:\n",
        "                real_unlabeled_idx.append(count)\n",
        "            count=count+1\n",
        "        with tf.GradientTape() as gen_tape:\n",
        "            gout, real_activs, real_logits, real_outputs, fake_activs, fake_logits, fake_outputs=self.call(real_batch, training=True)\n",
        "            gen_loss=self.gen.gen_loss(real_activs, fake_activs, fake_outputs)\n",
        "        gradients_gen=gen_tape.gradient(gen_loss, self.gen.trainable_variables)\n",
        "        self.gen_opt.apply_gradients(zip(gradients_gen, self.gen.trainable_variables))\n",
        "        with tf.GradientTape() as disc_tape:\n",
        "            gout, real_activs, real_logits, real_outputs, fake_activs, fake_logits, fake_outputs=self.call(real_batch, training=True)\n",
        "            disc_loss=self.disc.disc_loss(real_logits, real_outputs, real_targets, True, fake_outputs, label_mask)\n",
        "        gradients_disc=disc_tape.gradient(disc_loss, self.disc.trainable_variables)\n",
        "        self.disc_opt.apply_gradients(zip(gradients_disc, self.disc.trainable_variables))\n",
        "        masked_real_outputs=tf.boolean_mask(real_outputs, label_mask)\n",
        "        masked_real_targets=tf.boolean_mask(real_targets, label_mask)\n",
        "        if masked_real_targets.shape[0] != 0:\n",
        "            preds=masked_real_outputs[:,0:-1]\n",
        "            masked_real_targets=tf.one_hot(masked_real_targets, num_classes)\n",
        "            self.sup_acc.update_state(masked_real_targets, preds)\n",
        "            self.previous_supervised_accuracy = self.sup_acc.result().numpy()\n",
        "        if len(real_unlabeled_idx) != 0:\n",
        "            real_unlabeled_targets=tf.ones_like(tf.gather(real_targets, real_unlabeled_idx))\n",
        "            fake_unlabeled_targets=tf.zeros_like(self.fake_targets)\n",
        "            real_unlabeled_logits=tf.gather(real_logits, real_unlabeled_idx)\n",
        "            unlabeled_logits=tf.concat([real_unlabeled_logits, fake_logits], axis=0)\n",
        "            unlabeled_targets=tf.concat([real_unlabeled_targets, fake_unlabeled_targets], axis=0)\n",
        "            randomIdx=tf.random.shuffle(tf.range(unlabeled_targets.shape[0]))\n",
        "            unlabeled_targets=tf.gather(unlabeled_targets, randomIdx)\n",
        "            unlabeled_logits=tf.gather(unlabeled_logits, randomIdx)\n",
        "            probs=tf.nn.sigmoid(unlabeled_logits[:,-1])\n",
        "            self.unsup_acc.update_state(tf.cast(unlabeled_targets, tf.float32), probs)\n",
        "            self.previous_unsupervised_accuracy = self.unsup_acc.result().numpy()\n",
        "        return {\"gen_loss\": gen_loss.numpy(), \"disc_loss\": disc_loss.numpy(), \"supervised accuracy\":self.sup_acc.result().numpy() if masked_real_targets.shape[0] != 0 else self.previous_supervised_accuracy, \"unsupervised accuracy\":self.unsup_acc.result().numpy() if len(real_unlabeled_idx) != 0 else self.previous_unsupervised_accuracy}\n",
        "    def test_step(self, data):\n",
        "        samples, targets=data\n",
        "        test_activs, test_logits, test_outputs=self.call(samples, training=False)\n",
        "        loss=self.disc.disc_loss(test_logits, test_outputs, targets, training=False)\n",
        "        pred_test=test_outputs[:,0:-1]\n",
        "        targets=tf.one_hot(targets, num_classes)\n",
        "        self.sup_acc.update_state(targets, pred_test)\n",
        "        return {\"loss\": loss.numpy(), \"accuracy\": self.sup_acc.result().numpy()}"
      ],
      "metadata": {
        "id": "9oTAuUV3Vtvb",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:12.914826Z",
          "iopub.execute_input": "2024-06-30T13:07:12.915240Z",
          "iopub.status.idle": "2024-06-30T13:07:12.959621Z",
          "shell.execute_reply.started": "2024-06-30T13:07:12.915207Z",
          "shell.execute_reply": "2024-06-30T13:07:12.958504Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test=GANBERT()"
      ],
      "metadata": {
        "id": "ppN1VoxCCLhH",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:13.924807Z",
          "iopub.execute_input": "2024-06-30T13:07:13.925748Z",
          "iopub.status.idle": "2024-06-30T13:07:20.246455Z",
          "shell.execute_reply.started": "2024-06-30T13:07:13.925706Z",
          "shell.execute_reply": "2024-06-30T13:07:20.245441Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "semi=test.preparing_dataset(trainX, trainY, unlabX, unlabY, valX, valY, testX, testY)"
      ],
      "metadata": {
        "id": "hm8c0c1DN17E",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:22.953076Z",
          "iopub.execute_input": "2024-06-30T13:07:22.953971Z",
          "iopub.status.idle": "2024-06-30T13:07:39.837916Z",
          "shell.execute_reply.started": "2024-06-30T13:07:22.953921Z",
          "shell.execute_reply": "2024-06-30T13:07:39.836874Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_optimizer = tf_keras.optimizers.Adam(learning_rate=1e-5)\n",
        "disc_optimizer = tf_keras.optimizers.Adam(learning_rate=1e-5)\n",
        "if num_classes == 2:\n",
        "    supervised_accuracy = tf_keras.metrics.BinaryAccuracy('supervised accuracy')\n",
        "elif num_classes == 5:\n",
        "    supervised_accuracy = tf_keras.metrics.CategoricalAccuracy('supervised accuracy')\n",
        "unsupervised_accuracy = tf_keras.metrics.BinaryAccuracy('unsupervised accuracy')\n",
        "test.compile(gen_optimizer, disc_optimizer, supervised_accuracy, unsupervised_accuracy)"
      ],
      "metadata": {
        "id": "8umu2kuHyjE3",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:45.886982Z",
          "iopub.execute_input": "2024-06-30T13:07:45.887339Z",
          "iopub.status.idle": "2024-06-30T13:07:45.934561Z",
          "shell.execute_reply.started": "2024-06-30T13:07:45.887308Z",
          "shell.execute_reply": "2024-06-30T13:07:45.933307Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=tf.data.Dataset.from_tensor_slices((semi[0], semi[1])).shuffle(semi[0][0].shape[0]).batch(32)\n",
        "val_dataset=tf.data.Dataset.from_tensor_slices((semi[2], semi[3])).shuffle(semi[2][0].shape[0]).batch(32)\n",
        "test_dataset=tf.data.Dataset.from_tensor_slices((semi[4], semi[5])).shuffle(semi[4][0].shape[0]).batch(32)"
      ],
      "metadata": {
        "id": "KX7H9TtTN17G",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:48.041922Z",
          "iopub.execute_input": "2024-06-30T13:07:48.042266Z",
          "iopub.status.idle": "2024-06-30T13:07:48.066638Z",
          "shell.execute_reply.started": "2024-06-30T13:07:48.042236Z",
          "shell.execute_reply": "2024-06-30T13:07:48.065771Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    test.reset_metrics()\n",
        "    history=test.fit(train_dataset, batch_size=32, epochs=1, shuffle=True)\n",
        "    test.reset_metrics()\n",
        "    test.evaluate(val_dataset)"
      ],
      "metadata": {
        "id": "qOWZCo1w1__i",
        "execution": {
          "iopub.status.busy": "2024-06-30T13:07:55.282066Z",
          "iopub.execute_input": "2024-06-30T13:07:55.283182Z",
          "iopub.status.idle": "2024-06-30T14:14:35.013856Z",
          "shell.execute_reply.started": "2024-06-30T13:07:55.283140Z",
          "shell.execute_reply": "2024-06-30T14:14:35.012802Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.reset_metrics()\n",
        "results = test.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "KrKcV_fG1__j",
        "execution": {
          "iopub.status.busy": "2024-06-30T14:14:37.517692Z",
          "iopub.execute_input": "2024-06-30T14:14:37.518429Z",
          "iopub.status.idle": "2024-06-30T14:15:28.516524Z",
          "shell.execute_reply.started": "2024-06-30T14:14:37.518398Z",
          "shell.execute_reply": "2024-06-30T14:15:28.515634Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FROM HERE STARTS THE SUPERVISED PART, RUN ALL THE CELLS BELOW\n",
        "train_df['Cleaned']=train_df[train_df.columns[0]].apply(clean_text)\n",
        "val_df['Cleaned']=val_df[train_df.columns[0]].apply(clean_text)\n",
        "test_df['Cleaned']=test_df[train_df.columns[0]].apply(clean_text)"
      ],
      "metadata": {
        "id": "fHStLyAGB2zQ",
        "execution": {
          "iopub.status.busy": "2024-06-29T22:25:53.602494Z",
          "iopub.execute_input": "2024-06-29T22:25:53.603346Z",
          "iopub.status.idle": "2024-06-29T22:25:54.499181Z",
          "shell.execute_reply.started": "2024-06-29T22:25:53.603307Z",
          "shell.execute_reply": "2024-06-29T22:25:54.498037Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainX=train_df['Cleaned']\n",
        "trainY=tf.convert_to_tensor(train_df[train_df.columns[1]])\n",
        "valX=val_df['Cleaned']\n",
        "valY=tf.convert_to_tensor(val_df[val_df.columns[1]])\n",
        "testX=test_df['Cleaned']\n",
        "testY=tf.convert_to_tensor(test_df[test_df.columns[1]])"
      ],
      "metadata": {
        "id": "N9x4Mk6h1__j",
        "execution": {
          "iopub.status.busy": "2024-06-29T22:33:53.389156Z",
          "iopub.execute_input": "2024-06-29T22:33:53.389622Z",
          "iopub.status.idle": "2024-06-29T22:33:53.397750Z",
          "shell.execute_reply.started": "2024-06-29T22:33:53.389590Z",
          "shell.execute_reply": "2024-06-29T22:33:53.396561Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=64\n",
        "tz=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "trainX_encoded=tz(trainX.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "valX_encoded=tz(valX.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "testX_encoded=tz(testX.tolist(), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')"
      ],
      "metadata": {
        "id": "4g-mpDGM1__j",
        "execution": {
          "iopub.status.busy": "2024-06-29T22:33:56.352161Z",
          "iopub.execute_input": "2024-06-29T22:33:56.352822Z",
          "iopub.status.idle": "2024-06-29T22:34:03.863337Z",
          "shell.execute_reply.started": "2024-06-29T22:33:56.352790Z",
          "shell.execute_reply": "2024-06-29T22:34:03.862499Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred_logits, num_classes=num_classes):\n",
        "    log_probs=tf.nn.log_softmax(y_pred_logits[:,0:num_classes])\n",
        "    per_data_sup_loss=-tf.reduce_sum(y_true*log_probs)\n",
        "    loss=tf.reduce_mean(per_data_sup_loss)\n",
        "    return loss"
      ],
      "metadata": {
        "id": "A09hvPtXB2zR",
        "execution": {
          "iopub.status.busy": "2024-06-29T22:34:13.671694Z",
          "iopub.execute_input": "2024-06-29T22:34:13.672073Z",
          "iopub.status.idle": "2024-06-29T22:34:13.677452Z",
          "shell.execute_reply.started": "2024-06-29T22:34:13.672044Z",
          "shell.execute_reply": "2024-06-29T22:34:13.676327Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY=tf.one_hot(trainY, num_classes)\n",
        "valY=tf.one_hot(valY, num_classes)\n",
        "testY=tf.one_hot(testY, num_classes)"
      ],
      "metadata": {
        "id": "ZOcVVOPcB2zS",
        "execution": {
          "iopub.status.busy": "2024-06-29T22:34:25.042446Z",
          "iopub.execute_input": "2024-06-29T22:34:25.042883Z",
          "iopub.status.idle": "2024-06-29T22:34:25.051089Z",
          "shell.execute_reply.started": "2024-06-29T22:34:25.042852Z",
          "shell.execute_reply": "2024-06-29T22:34:25.049868Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt=tf_keras.optimizers.Adam(learning_rate=3e-5)\n",
        "callback=tf_keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=5e-4, patience=10, restore_best_weights=True)\n",
        "if num_classes == 2:\n",
        "    metric=tf_keras.metrics.BinaryAccuracy('accuracy')\n",
        "elif num_classes == 5:\n",
        "    metric=tf_keras.metrics.CategoricalAccuracy('accuracy')\n",
        "num_epochs=100\n",
        "config=BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "dropout_rate=0.3\n",
        "config.hidden_dropout_prob=dropout_rate\n",
        "config.num_labels=num_classes\n",
        "model=TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
        "model.compile(optimizer=opt, loss=custom_loss, metrics=[metric])"
      ],
      "metadata": {
        "id": "86BX9JAa1__k",
        "execution": {
          "iopub.status.busy": "2024-06-04T17:10:40.986578Z",
          "iopub.execute_input": "2024-06-04T17:10:40.986993Z",
          "iopub.status.idle": "2024-06-04T17:10:45.545060Z",
          "shell.execute_reply.started": "2024-06-04T17:10:40.986961Z",
          "shell.execute_reply": "2024-06-04T17:10:45.543911Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=model.fit([trainX_encoded['input_ids'], trainX_encoded['attention_mask']], trainY, batch_size=32, epochs=num_epochs, verbose=1, shuffle=True, callbacks=[callback], validation_data=([valX_encoded['input_ids'], valX_encoded['attention_mask']], valY))"
      ],
      "metadata": {
        "id": "Fz8sQVZvIiwY",
        "execution": {
          "iopub.status.busy": "2024-06-04T17:10:47.517460Z",
          "iopub.execute_input": "2024-06-04T17:10:47.517859Z",
          "iopub.status.idle": "2024-06-04T17:26:20.088739Z",
          "shell.execute_reply.started": "2024-06-04T17:10:47.517827Z",
          "shell.execute_reply": "2024-06-04T17:26:20.087821Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate([testX_encoded['input_ids'], testX_encoded['attention_mask']], testY)\n",
        "print(f'Test loss: {test_loss}, Test accuracy: {test_accuracy}')"
      ],
      "metadata": {
        "id": "8BZyh-qgB2zT",
        "execution": {
          "iopub.status.busy": "2024-06-04T17:26:23.424619Z",
          "iopub.execute_input": "2024-06-04T17:26:23.425004Z",
          "iopub.status.idle": "2024-06-04T17:27:01.380560Z",
          "shell.execute_reply.started": "2024-06-04T17:26:23.424974Z",
          "shell.execute_reply": "2024-06-04T17:27:01.379291Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}